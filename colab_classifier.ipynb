{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API LLM Detector - Google Colab\n",
    "\n",
    "Detect which chat API (ChatGPT, Claude, Grok, Gemini, or DeepSeek) generated a piece of text.\n",
    "\n",
    "**Important:** Make sure to enable GPU runtime!\n",
    "- Go to: Runtime → Change runtime type → Hardware accelerator → GPU (T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llm2vec==0.2.3 huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download the Classifier\n",
    "\n",
    "This will download ~16GB. It may take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download Yida/classifier_chat --local-dir ./classifier_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from llm2vec import LLM2Vec\n",
    "\n",
    "def load_classifier(checkpoint_path=\"./classifier_chat\", num_labels=5):\n",
    "    \"\"\"Load the pre-trained LLM2Vec classifier.\"\"\"\n",
    "    print(\"Loading classifier...\")\n",
    "    \n",
    "    base_model_name = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        base_model_name,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        f\"{base_model_name}-supervised\",\n",
    "        is_trainable=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
    "    \n",
    "    hidden_size = list(model.modules())[-1].weight.shape[0]\n",
    "    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n",
    "    \n",
    "    import os\n",
    "    head_path = os.path.join(checkpoint_path, \"head.pt\")\n",
    "    model.head.load_state_dict(torch.load(head_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"✓ Classifier loaded successfully! Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "model = load_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, text):\n",
    "    \"\"\"Predict which LLM generated the given text.\"\"\"\n",
    "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "    \n",
    "    # Prepare text\n",
    "    prepared_text = model.prepare_for_tokenization(text)\n",
    "    inputs = model.tokenize([prepared_text])\n",
    "    \n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.forward(inputs).to(torch.bfloat16)\n",
    "        logits = model.head(embeddings)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    pred_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    all_probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nMost likely source: {label_names[pred_label]}\")\n",
    "    print(f\"Confidence: {all_probs[pred_label]*100:.2f}%\")\n",
    "    print(\"\\nAll probabilities:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    sorted_indices = np.argsort(all_probs)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        bar_length = int(all_probs[idx] * 50)\n",
    "        bar = \"█\" * bar_length\n",
    "        print(f\"{label_names[idx]:20s} {all_probs[idx]*100:6.2f}% {bar}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return label_names[pred_label], all_probs[pred_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test with Sample Text\n",
    "\n",
    "Replace the text below with any text you want to classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text - replace with your own!\n",
    "sample_text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "3. Finally, let's look at practical applications\n",
    "\n",
    "I hope this helps clarify things for you!\n",
    "\"\"\"\n",
    "\n",
    "predict_text(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Classify Your Own Text\n",
    "\n",
    "Paste your text in the input box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive input\n",
    "print(\"Paste your text below and press Enter:\")\n",
    "user_text = input()\n",
    "\n",
    "if user_text.strip():\n",
    "    predict_text(model, user_text)\n",
    "else:\n",
    "    print(\"No text provided!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Texts\n",
    "\n",
    "You can test multiple texts at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple texts\n",
    "texts_to_test = [\n",
    "    \"Sure, I can help with that!\",\n",
    "    \"I'd be happy to assist you with this question.\",\n",
    "    \"Let me break this down for you step by step.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(texts_to_test, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEXT #{i}: {text[:50]}...\")\n",
    "    predict_text(model, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
