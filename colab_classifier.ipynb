{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API LLM Detector - Google Colab\n",
    "\n",
    "Detect which chat API (ChatGPT, Claude, Grok, Gemini, or DeepSeek) generated a piece of text.\n",
    "\n",
    "**Important:** Make sure to enable GPU runtime!\n",
    "- Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies\n\nInstalling compatible versions of all required packages...\n\n**Note:** This project uses Llama 3, which requires Hugging Face authentication."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install compatible versions to avoid dependency conflicts\n!pip install -q transformers==4.46.3 peft==0.13.2 huggingface-hub accelerate\n!pip install -q llm2vec==0.2.3\n\nprint(\"‚úì Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Download the Classifier\n\nThis will download ~16GB. It may take 5-10 minutes."
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import login\n\n# Try to get token from Colab secrets\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    login(token=HF_TOKEN)\n    print(\"‚úì Successfully authenticated with Hugging Face!\")\nexcept:\n    # If secret fails, try manual input\n    print(\"‚ö†Ô∏è  Could not find HF_TOKEN in Colab secrets.\")\n    print(\"\\nOption 1 - Add to Colab Secrets (recommended):\")\n    print(\"1. Request access: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\")\n    print(\"2. Create token: https://huggingface.co/settings/tokens\")\n    print(\"3. Click the üîë icon on the left sidebar\")\n    print(\"4. Add a secret named 'HF_TOKEN' with your token\")\n    print(\"5. Enable notebook access toggle\")\n    print(\"6. Restart runtime and re-run this cell\")\n    print(\"\\nOption 2 - Manual login (less secure):\")\n    from getpass import getpass\n    token = getpass(\"Enter your HuggingFace token: \")\n    login(token=token)\n    print(\"‚úì Authenticated!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Load the Classifier",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download Yida/classifier_chat --local-dir ./classifier_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Define Prediction Function"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom peft import PeftModel\nfrom llm2vec import LLM2Vec\nimport gc\nimport os\n\ndef load_classifier(checkpoint_path=\"./classifier_chat\", num_labels=5):\n    \"\"\"Load the pre-trained LLM2Vec classifier with memory optimization.\"\"\"\n    print(\"Loading classifier with memory optimization...\")\n    \n    # Clear cache first\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    base_model_name = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n    \n    config = AutoConfig.from_pretrained(\n        base_model_name,\n        trust_remote_code=True,\n    )\n    \n    # Strategy: Try single GPU first, fall back to multi-GPU if OOM\n    num_gpus = torch.cuda.device_count()\n    print(f\"Detected {num_gpus} GPU(s)\")\n    \n    # Always use single GPU - multi-GPU causes device mismatch with residual connections\n    # Even with 2 GPUs available, using just one with CPU offload is more reliable\n    print(\"Using single GPU (GPU 0) with CPU offload for stability\")\n    device_map = {\"\": 0}  # Force everything to GPU 0\n    max_memory = {0: \"14GiB\", \"cpu\": \"30GiB\"}\n    \n    # Load base model\n    print(\"Loading base model...\")\n    model = AutoModel.from_pretrained(\n        base_model_name,\n        config=config,\n        torch_dtype=torch.bfloat16,\n        device_map=device_map,\n        max_memory=max_memory,\n        offload_folder=\"./offload\",\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(\"‚úì Base model loaded\")\n    torch.cuda.empty_cache()\n    \n    # Load and merge first adapter\n    print(\"Loading first adapter...\")\n    model = PeftModel.from_pretrained(\n        model,\n        base_model_name,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n    )\n    \n    # Move to CPU for merging to avoid OOM\n    print(\"‚úì Moving to CPU for merging...\")\n    model = model.cpu()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print(\"‚úì Merging first adapter...\")\n    model = model.merge_and_unload()\n    \n    # Move back to GPU 0 with CPU offload\n    print(\"‚úì Moving merged model back to GPU...\")\n    model = model.to(torch.bfloat16)\n    \n    # Re-dispatch to GPU 0 (with CPU offload for layers that don't fit)\n    from accelerate import dispatch_model, infer_auto_device_map\n    device_map_merged = infer_auto_device_map(\n        model,\n        max_memory=max_memory,\n        no_split_module_classes=[\"LlamaDecoderLayer\"]  # Keep decoder layers intact\n    )\n    model = dispatch_model(model, device_map=device_map_merged, offload_dir=\"./offload\")\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Load supervised adapter\n    print(\"‚úì Loading supervised adapter...\")\n    model = PeftModel.from_pretrained(\n        model,\n        f\"{base_model_name}-supervised\",\n        is_trainable=True,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n    )\n    \n    torch.cuda.empty_cache()\n    \n    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n    \n    hidden_size = list(model.modules())[-1].weight.shape[0]\n    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n    \n    head_path = os.path.join(checkpoint_path, \"head.pt\")\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.head.load_state_dict(torch.load(head_path, map_location=device))\n    model.head = model.head.to(device)\n    \n    model.eval()\n    \n    # Final cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Show memory usage across all GPUs\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            reserved = torch.cuda.memory_reserved(i) / 1024**3\n            if i == 0 or allocated > 0:\n                print(f\"‚úì GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    else:\n        print(\"‚úì Classifier loaded on CPU\")\n    \n    return model\n\n# Load the model\nmodel = load_classifier()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test with Sample Text\n\nReplace the text below with any text you want to classify:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_text(model, text):\n    \"\"\"Predict which LLM generated the given text.\"\"\"\n    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n    \n    # Prepare text\n    prepared_text = model.prepare_for_tokenization(text)\n    inputs = model.tokenize([prepared_text])\n    \n    # IMPORTANT: For multi-GPU setups, always put inputs on cuda:0\n    # The model will handle moving tensors between GPUs automatically\n    if torch.cuda.is_available():\n        target_device = torch.device(\"cuda:0\")\n    else:\n        target_device = next(model.parameters()).device\n    \n    inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n    \n    # Predict\n    with torch.no_grad():\n        embeddings = model.forward(inputs)\n        \n        # Move embeddings to same device as classification head\n        if hasattr(model, 'head'):\n            head_device = next(model.head.parameters()).device\n            embeddings = embeddings.to(head_device)\n        \n        embeddings = embeddings.to(torch.bfloat16)\n        logits = model.head(embeddings)\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    \n    pred_label = torch.argmax(probabilities, dim=-1).item()\n    # Convert to float32 before numpy (bfloat16 not supported by numpy)\n    all_probs = probabilities[0].float().cpu().numpy()\n    \n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"PREDICTION RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\nMost likely source: {label_names[pred_label]}\")\n    print(f\"Confidence: {all_probs[pred_label]*100:.2f}%\")\n    print(\"\\nAll probabilities:\")\n    print(\"-\"*60)\n    \n    sorted_indices = np.argsort(all_probs)[::-1]\n    for idx in sorted_indices:\n        bar_length = int(all_probs[idx] * 50)\n        bar = \"‚ñà\" * bar_length\n        print(f\"{label_names[idx]:20s} {all_probs[idx]*100:6.2f}% {bar}\")\n    print(\"=\"*60)\n    \n    return label_names[pred_label], all_probs[pred_label]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Classify Your Own Text\n\nPaste your text in the input box below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text - replace with your own!\n",
    "sample_text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "3. Finally, let's look at practical applications\n",
    "\n",
    "I hope this helps clarify things for you!\n",
    "\"\"\"\n",
    "\n",
    "predict_text(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Classify Your Own Text\n",
    "\n",
    "Paste your text in the input box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive input\n",
    "print(\"Paste your text below and press Enter:\")\n",
    "user_text = input()\n",
    "\n",
    "if user_text.strip():\n",
    "    predict_text(model, user_text)\n",
    "else:\n",
    "    print(\"No text provided!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Texts\n",
    "\n",
    "You can test multiple texts at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple texts\n",
    "texts_to_test = [\n",
    "    \"Sure, I can help with that!\",\n",
    "    \"I'd be happy to assist you with this question.\",\n",
    "    \"Let me break this down for you step by step.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(texts_to_test, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEXT #{i}: {text[:50]}...\")\n",
    "    predict_text(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "source": "!pip install -q gradio pandas\n\nimport gradio as gr\nimport pandas as pd\nimport io\nimport sys\nfrom contextlib import redirect_stdout, redirect_stderr\n\ndef predict_gradio(text):\n    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n    if not text.strip():\n        return \"‚ö†Ô∏è Please enter some text to classify.\", None, \"‚ö†Ô∏è No text provided\"\n    \n    # Capture all output (stdout and stderr)\n    log_capture = io.StringIO()\n    \n    try:\n        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n        \n        # Start logging\n        log_capture.write(\"üîÑ Starting prediction...\\n\\n\")\n        \n        # Prepare text\n        log_capture.write(\"üìù Preparing text for tokenization...\\n\")\n        prepared_text = model.prepare_for_tokenization(text)\n        log_capture.write(f\"‚úì Text prepared (length: {len(text)} chars)\\n\\n\")\n        \n        # Tokenize\n        log_capture.write(\"üî§ Tokenizing input...\\n\")\n        inputs = model.tokenize([prepared_text])\n        log_capture.write(f\"‚úì Tokenization complete\\n\\n\")\n        \n        # Move to device\n        log_capture.write(\"üñ•Ô∏è  Moving tensors to device...\\n\")\n        if torch.cuda.is_available():\n            target_device = torch.device(\"cuda:0\")\n            log_capture.write(f\"‚úì Using GPU: cuda:0\\n\\n\")\n        else:\n            target_device = next(model.parameters()).device\n            log_capture.write(f\"‚úì Using device: {target_device}\\n\\n\")\n        \n        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n        \n        # Predict\n        log_capture.write(\"üß† Running model inference...\\n\")\n        with torch.no_grad():\n            embeddings = model.forward(inputs)\n            log_capture.write(f\"‚úì Generated embeddings (shape: {embeddings.shape})\\n\")\n            \n            if hasattr(model, 'head'):\n                head_device = next(model.head.parameters()).device\n                embeddings = embeddings.to(head_device)\n                log_capture.write(f\"‚úì Moved embeddings to classification head device\\n\")\n            \n            embeddings = embeddings.to(torch.bfloat16)\n            logits = model.head(embeddings)\n            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n            log_capture.write(f\"‚úì Computed probabilities\\n\\n\")\n        \n        pred_label = torch.argmax(probabilities, dim=-1).item()\n        all_probs = probabilities[0].float().cpu().numpy()\n        \n        # Log results\n        log_capture.write(\"=\"*60 + \"\\n\")\n        log_capture.write(\"üìä PREDICTION RESULTS\\n\")\n        log_capture.write(\"=\"*60 + \"\\n\\n\")\n        log_capture.write(f\"üéØ Most likely source: {label_names[pred_label]}\\n\")\n        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.2f}%\\n\\n\")\n        log_capture.write(\"üìà All probabilities:\\n\")\n        log_capture.write(\"-\"*60 + \"\\n\")\n        \n        sorted_indices = np.argsort(all_probs)[::-1]\n        for idx in sorted_indices:\n            bar_length = int(all_probs[idx] * 40)\n            bar = \"‚ñà\" * bar_length\n            log_capture.write(f\"{label_names[idx]:20s} {all_probs[idx]*100:6.2f}% {bar}\\n\")\n        \n        log_capture.write(\"=\"*60 + \"\\n\\n\")\n        log_capture.write(\"‚úÖ Prediction complete!\\n\")\n        \n        # Create results for display\n        result_text = f\"## üéØ Prediction: **{label_names[pred_label]}**\\n\\n\"\n        result_text += f\"### Confidence: **{all_probs[pred_label]*100:.2f}%**\\n\\n\"\n        result_text += \"---\\n\\n\"\n        result_text += \"### All Probabilities:\\n\\n\"\n        \n        for idx in sorted_indices:\n            result_text += f\"- **{label_names[idx]}**: {all_probs[idx]*100:.2f}%\\n\"\n        \n        # Create probability distribution for plot - using pandas DataFrame\n        prob_data = pd.DataFrame({\n            'Model': label_names,\n            'Probability': [float(all_probs[i]) for i in range(len(label_names))]\n        })\n        # Sort by probability for better visualization\n        prob_data = prob_data.sort_values('Probability', ascending=False)\n        \n        # Get all logs\n        all_logs = log_capture.getvalue()\n        \n        return result_text, prob_data, all_logs\n        \n    except Exception as e:\n        import traceback\n        error_msg = f\"‚ùå Error during prediction: {str(e)}\\n\\n\"\n        error_msg += f\"Error type: {type(e).__name__}\\n\\n\"\n        error_msg += f\"Traceback:\\n{traceback.format_exc()}\\n\"\n        log_capture.write(error_msg)\n        return f\"‚ùå Error: {str(e)}\", pd.DataFrame(), log_capture.getvalue()\n\n\n# Create Gradio interface with logs\nwith gr.Blocks(title=\"LLM Detector\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\n        \"\"\"\n        # üîç Chat API LLM Detector\n        \n        Detect which AI model (ChatGPT, Claude, Grok, Gemini, or DeepSeek) generated a piece of text.\n        \n        **Accuracy: 97.1%** | Based on [research paper](https://arxiv.org/abs/2502.12150)\n        \"\"\"\n    )\n    \n    with gr.Row():\n        with gr.Column(scale=2):\n            text_input = gr.Textbox(\n                label=\"Enter text to classify\",\n                placeholder=\"Paste the AI-generated text here...\",\n                lines=8,\n            )\n            \n            submit_btn = gr.Button(\"üîç Detect LLM\", variant=\"primary\", size=\"lg\")\n            \n            gr.Examples(\n                examples=[\n                    [\"Hello! I'd be happy to help you with that question. Let me break this down into a few key points.\"],\n                    [\"Sure, I can help with that! Here's what you need to know...\"],\n                    [\"I'd be delighted to assist you with this matter.\"],\n                ],\n                inputs=text_input,\n            )\n    \n    # Results section\n    gr.Markdown(\"## üìä Results\")\n    \n    with gr.Row():\n        with gr.Column(scale=1):\n            result_output = gr.Markdown(value=\"\", label=\"Prediction Results\")\n        \n        with gr.Column(scale=1):\n            plot_output = gr.BarPlot(\n                value=pd.DataFrame({'Model': [], 'Probability': []}),\n                x=\"Model\",\n                y=\"Probability\", \n                title=\"Prediction Confidence\",\n                y_lim=[0, 1],\n                height=300,\n                width=400,\n            )\n    \n    # Logs section\n    gr.Markdown(\"## üìã Processing Logs\")\n    logs_output = gr.Textbox(\n        value=\"\",\n        label=\"Detailed logs\",\n        lines=15,\n        max_lines=20,\n        show_copy_button=True,\n        interactive=False,\n    )\n    \n    # Connect the button\n    submit_btn.click(\n        fn=predict_gradio,\n        inputs=text_input,\n        outputs=[result_output, plot_output, logs_output]\n    )\n    \n    # Also trigger on enter\n    text_input.submit(\n        fn=predict_gradio,\n        inputs=text_input,\n        outputs=[result_output, plot_output, logs_output]\n    )\n    \n    gr.Markdown(\n        \"\"\"\n        ---\n        **Note:** Works best on unedited AI-generated text. Accuracy decreases for short text (< 50 words).\n        \"\"\"\n    )\n\n# Launch the app\ndemo.launch(share=True, debug=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Interactive Gradio UI\n\nLaunch an interactive web interface to classify text:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}