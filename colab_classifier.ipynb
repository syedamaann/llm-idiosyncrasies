{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API LLM Detector - Google Colab\n",
    "\n",
    "Detect which chat API (ChatGPT, Claude, Grok, Gemini, or DeepSeek) generated a piece of text.\n",
    "\n",
    "**Important:** Make sure to enable GPU runtime!\n",
    "- Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies\n\nInstalling compatible versions of all required packages...\n\n**Note:** This project uses Llama 3, which requires Hugging Face authentication."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install compatible versions to avoid dependency conflicts\n!pip install -q transformers==4.46.3 peft==0.13.2 huggingface-hub accelerate\n!pip install -q llm2vec==0.2.3\n\nprint(\"‚úì Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Download the Classifier\n\nThis will download ~16GB. It may take 5-10 minutes."
  },
  {
   "cell_type": "code",
   "source": "from google.colab import userdata\nfrom huggingface_hub import login\n\ntry:\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    login(token=HF_TOKEN)\n    print(\"‚úì Successfully authenticated with Hugging Face!\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è  Could not find HF_TOKEN in Colab secrets.\")\n    print(\"\\nPlease follow these steps:\")\n    print(\"1. Request access: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\")\n    print(\"2. Create token: https://huggingface.co/settings/tokens\")\n    print(\"3. Click the üîë icon on the left sidebar\")\n    print(\"4. Add a secret named 'HF_TOKEN' with your token\")\n    print(\"5. Enable notebook access\")\n    print(\"\\nThen re-run this cell.\")\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Load the Classifier",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download Yida/classifier_chat --local-dir ./classifier_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Define Prediction Function"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom peft import PeftModel\nfrom llm2vec import LLM2Vec\nimport gc\n\ndef load_classifier(checkpoint_path=\"./classifier_chat\", num_labels=5):\n    \"\"\"Load the pre-trained LLM2Vec classifier with memory optimization.\"\"\"\n    print(\"Loading classifier with memory optimization...\")\n    \n    # Clear cache first\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    base_model_name = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n    \n    config = AutoConfig.from_pretrained(\n        base_model_name,\n        trust_remote_code=True,\n    )\n    \n    # Load with low_cpu_mem_usage to reduce memory footprint\n    model = AutoModel.from_pretrained(\n        base_model_name,\n        config=config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",  # Auto device mapping for better memory management\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(\"‚úì Base model loaded\")\n    torch.cuda.empty_cache()\n    \n    # Load first adapter\n    model = PeftModel.from_pretrained(\n        model,\n        base_model_name,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    \n    print(\"‚úì Merging first adapter...\")\n    model = model.merge_and_unload()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Load supervised adapter\n    print(\"‚úì Loading supervised adapter...\")\n    model = PeftModel.from_pretrained(\n        model,\n        f\"{base_model_name}-supervised\",\n        is_trainable=True,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    \n    torch.cuda.empty_cache()\n    \n    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n    \n    hidden_size = list(model.modules())[-1].weight.shape[0]\n    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n    \n    import os\n    head_path = os.path.join(checkpoint_path, \"head.pt\")\n    device = next(model.parameters()).device\n    model.head.load_state_dict(torch.load(head_path, map_location=device))\n    model.head = model.head.to(device)\n    \n    model.eval()\n    \n    # Final cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Show memory usage\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1024**3\n        reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"‚úì Classifier loaded! GPU memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    else:\n        print(\"‚úì Classifier loaded on CPU\")\n    \n    return model\n\n# Load the model\nmodel = load_classifier()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test with Sample Text\n\nReplace the text below with any text you want to classify:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, text):\n",
    "    \"\"\"Predict which LLM generated the given text.\"\"\"\n",
    "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "    \n",
    "    # Prepare text\n",
    "    prepared_text = model.prepare_for_tokenization(text)\n",
    "    inputs = model.tokenize([prepared_text])\n",
    "    \n",
    "    # Move to device\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.forward(inputs).to(torch.bfloat16)\n",
    "        logits = model.head(embeddings)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    pred_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    all_probs = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nMost likely source: {label_names[pred_label]}\")\n",
    "    print(f\"Confidence: {all_probs[pred_label]*100:.2f}%\")\n",
    "    print(\"\\nAll probabilities:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    sorted_indices = np.argsort(all_probs)[::-1]\n",
    "    for idx in sorted_indices:\n",
    "        bar_length = int(all_probs[idx] * 50)\n",
    "        bar = \"‚ñà\" * bar_length\n",
    "        print(f\"{label_names[idx]:20s} {all_probs[idx]*100:6.2f}% {bar}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return label_names[pred_label], all_probs[pred_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Classify Your Own Text\n\nPaste your text in the input box below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text - replace with your own!\n",
    "sample_text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "3. Finally, let's look at practical applications\n",
    "\n",
    "I hope this helps clarify things for you!\n",
    "\"\"\"\n",
    "\n",
    "predict_text(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Classify Your Own Text\n",
    "\n",
    "Paste your text in the input box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive input\n",
    "print(\"Paste your text below and press Enter:\")\n",
    "user_text = input()\n",
    "\n",
    "if user_text.strip():\n",
    "    predict_text(model, user_text)\n",
    "else:\n",
    "    print(\"No text provided!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Texts\n",
    "\n",
    "You can test multiple texts at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple texts\n",
    "texts_to_test = [\n",
    "    \"Sure, I can help with that!\",\n",
    "    \"I'd be happy to assist you with this question.\",\n",
    "    \"Let me break this down for you step by step.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(texts_to_test, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEXT #{i}: {text[:50]}...\")\n",
    "    predict_text(model, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}