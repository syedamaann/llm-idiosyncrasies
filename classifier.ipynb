{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\n\nDetect which AI model (ChatGPT, Claude, Grok, Gemini, or DeepSeek) generated a piece of text.\n\n**Important:** Make sure to enable GPU runtime!\n\n**For Google Colab:**\n- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)\n\n**For Kaggle Notebooks:**\n- Settings ‚Üí Accelerator ‚Üí GPU P100"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies\n\nInstalling compatible versions of all required packages...\n\n**Note:** This project uses Llama 3, which requires Hugging Face authentication."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install compatible versions to avoid dependency conflicts\n!pip install -q transformers==4.46.3 peft==0.13.2 huggingface-hub accelerate\n!pip install -q llm2vec==0.2.3\n\nprint(\"‚úì Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Authenticate with Hugging Face\n\nThis model requires Hugging Face authentication. The notebook will try to use secrets from Colab or Kaggle."
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import login\n\n# Try to get token from secrets (works on Colab and Kaggle)\ntry:\n    # Try Colab secrets first\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    login(token=HF_TOKEN)\n    print(\"‚úì Successfully authenticated with Hugging Face!\")\nexcept:\n    try:\n        # Try Kaggle secrets\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n        login(token=HF_TOKEN)\n        print(\"‚úì Successfully authenticated with Hugging Face!\")\n    except:\n        # If both fail, show instructions\n        print(\"‚ö†Ô∏è  Could not find HF_TOKEN in secrets.\")\n        print(\"\\nüìù Setup Instructions:\")\n        print(\"\\n1. Request Llama 3 access: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\")\n        print(\"2. Create token: https://huggingface.co/settings/tokens\")\n        print(\"\\n3. Add to secrets:\")\n        print(\"   ‚Ä¢ Colab: Click üîë icon ‚Üí Add secret 'HF_TOKEN' ‚Üí Enable notebook access\")\n        print(\"   ‚Ä¢ Kaggle: Settings ‚Üí Add-ons ‚Üí Secrets ‚Üí Add 'HF_TOKEN'\")\n        print(\"\\n4. Restart runtime and re-run this cell\")\n        print(\"\\n‚öôÔ∏è  Alternative - Manual login:\")\n        from getpass import getpass\n        token = getpass(\"Enter your HuggingFace token: \")\n        login(token=token)\n        print(\"‚úì Authenticated!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Choose Loading Mode & Download\n\nChoose your loading strategy:\n- **FAST_FUSED**: Download pre-merged model (15.5GB) - Faster loading, higher disk usage\n- **LOW_BANDWIDTH**: Download only classification head (40KB) - Slower loading, minimal disk usage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Setup Configuration\n# Options: \"FAST_FUSED\" (Downloads 15.5GB, faster load) or \"LOW_BANDWIDTH\" (Downloads 40KB, slower load)\nLOAD_MODE = \"LOW_BANDWIDTH\" # @param [\"FAST_FUSED\", \"LOW_BANDWIDTH\"]\n\nimport os\n\n# Clear existing directory to avoid conflicts\n!rm -rf ./classifier_chat\n\n# Conditional Download\nif LOAD_MODE == \"FAST_FUSED\":\n    print(\"üöÄ Mode: FAST_FUSED selected.\")\n    print(\"Downloading pre-merged model weights (15.5 GB)...\")\n    # Downloads the full pre-merged model + head\n    !huggingface-cli download Yida/classifier_chat --include \"*.safetensors\" --include \"head.pt\" --local-dir ./classifier_chat\n    print(\"‚úì Download complete!\")\n\nelif LOAD_MODE == \"LOW_BANDWIDTH\":\n    print(\"üîß Mode: LOW_BANDWIDTH selected.\")\n    print(\"Downloading classification head only (42 KB)...\")\n    # Downloads ONLY the head; Base model + Adapters will be fetched from McGill-NLP later\n    !huggingface-cli download Yida/classifier_chat head.pt --local-dir ./classifier_chat\n    print(\"‚úì Download complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Load the Classifier\n\nDefine and load the classifier model with memory optimization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom peft import PeftModel\nfrom llm2vec import LLM2Vec\nimport gc\nimport os\n\ndef load_classifier(checkpoint_path=\"./classifier_chat\", mode=\"LOW_BANDWIDTH\", num_labels=5):\n    \"\"\"\n    Robust classifier loader with Dual-Pathway support.\n    mode=\"FAST_FUSED\": Loads pre-merged weights (Fast startup, High disk usage)\n    mode=\"LOW_BANDWIDTH\": Merges adapters at runtime (Slow startup, Low disk usage)\n    \"\"\"\n    print(f\"Loading classifier in {mode} mode...\")\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # Common Resource: The Base Model ID (Used for Config/Tokenizer in both modes)\n    base_model_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n\n    # Memory configuration\n    max_memory = {0: \"14GiB\", \"cpu\": \"30GiB\"}\n    \n    # =========================================================\n    # PATHWAY A: FAST / FUSED (Hybrid Load with Safety)\n    # =========================================================\n    if mode == \"FAST_FUSED\":\n        print(\"‚ö° Route: Loading pre-merged weights...\")\n\n        # 1. Load Configuration from Base (Critical Fix for missing config.json)\n        config = AutoConfig.from_pretrained(base_model_id, trust_remote_code=True)\n\n        # 2. Load Weights from Local Folder with SAFE device map\n        # NOTE: Using single GPU strategy to avoid residual connection device mismatch\n        device_map = {\"\": 0}  # Force everything to GPU 0\n        \n        model = AutoModel.from_pretrained(\n            checkpoint_path,\n            config=config,\n            torch_dtype=torch.bfloat16,\n            device_map=device_map,\n            max_memory=max_memory,\n            offload_folder=\"./offload\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n        )\n        \n        print(\"‚úì Pre-merged model loaded\")\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # =========================================================\n    # PATHWAY B: LOW BANDWIDTH / ADAPTER (Original Logic)\n    # =========================================================\n    else:\n        print(\"üîß Route: Building from adapters...\")\n\n        # 1. Load Base Model\n        print(\"Loading base model...\")\n        config = AutoConfig.from_pretrained(base_model_id, trust_remote_code=True)\n        \n        device_map = {\"\": 0}\n        model = AutoModel.from_pretrained(\n            base_model_id,\n            config=config,\n            torch_dtype=torch.bfloat16,\n            device_map=device_map,\n            max_memory=max_memory,\n            offload_folder=\"./offload\",\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n        )\n        \n        print(\"‚úì Base model loaded\")\n        torch.cuda.empty_cache()\n\n        # 2. Load and merge first adapter\n        print(\"Loading first adapter...\")\n        model = PeftModel.from_pretrained(\n            model,\n            base_model_id,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        # Move to CPU for merging to avoid OOM\n        print(\"‚úì Moving to CPU for merging...\")\n        model = model.cpu()\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        print(\"‚úì Merging first adapter...\")\n        model = model.merge_and_unload()\n\n        # Move back to GPU with CPU offload\n        print(\"‚úì Moving merged model back to GPU...\")\n        model = model.to(torch.bfloat16)\n\n        # Re-dispatch to GPU 0 (with CPU offload for layers that don't fit)\n        from accelerate import dispatch_model, infer_auto_device_map\n        device_map_merged = infer_auto_device_map(\n            model,\n            max_memory=max_memory,\n            no_split_module_classes=[\"LlamaDecoderLayer\"]  # Keep decoder layers intact\n        )\n        model = dispatch_model(model, device_map=device_map_merged, offload_dir=\"./offload\")\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        # 3. Load supervised adapter\n        print(\"‚úì Loading supervised adapter...\")\n        model = PeftModel.from_pretrained(\n            model,\n            f\"{base_model_id}-supervised\",\n            is_trainable=True,\n            torch_dtype=torch.bfloat16,\n            trust_remote_code=True,\n        )\n\n        torch.cuda.empty_cache()\n\n    # =========================================================\n    # COMMON: LLM2Vec & Classification Head\n    # =========================================================\n    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n\n    # Initialize Head\n    hidden_size = list(model.modules())[-1].weight.shape[0]\n    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n\n    # Load Head Weights with dynamic device detection\n    head_file = os.path.join(checkpoint_path, \"head.pt\")\n    try:\n        target_device = next(model.parameters()).device\n    except:\n        target_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    model.head.load_state_dict(torch.load(head_file, map_location=target_device))\n    model.head = model.head.to(target_device)\n\n    model.eval()\n    \n    # Final cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # Show memory usage\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            reserved = torch.cuda.memory_reserved(i) / 1024**3\n            if i == 0 or allocated > 0:\n                print(f\"‚úì GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n    else:\n        print(\"‚úì Classifier loaded on CPU\")\n\n    return model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the model (this may take a few minutes)\nprint(\"Loading model...\")\nmodel = load_classifier(mode=LOAD_MODE)\nprint(\"‚úì Model loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Define Prediction Function\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_text(model, text):\n    \"\"\"Predict which LLM generated the given text.\"\"\"\n    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n    \n    # Prepare text\n    prepared_text = model.prepare_for_tokenization(text)\n    inputs = model.tokenize([prepared_text])\n    \n    # IMPORTANT: For multi-GPU setups, always put inputs on cuda:0\n    # The model will handle moving tensors between GPUs automatically\n    if torch.cuda.is_available():\n        target_device = torch.device(\"cuda:0\")\n    else:\n        target_device = next(model.parameters()).device\n    \n    inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n    \n    # Predict\n    with torch.no_grad():\n        embeddings = model.forward(inputs)\n        \n        # Move embeddings to same device as classification head\n        if hasattr(model, 'head'):\n            head_device = next(model.head.parameters()).device\n            embeddings = embeddings.to(head_device)\n        \n        embeddings = embeddings.to(torch.bfloat16)\n        logits = model.head(embeddings)\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    \n    pred_label = torch.argmax(probabilities, dim=-1).item()\n    # Convert to float32 before numpy (bfloat16 not supported by numpy)\n    all_probs = probabilities[0].float().cpu().numpy()\n    \n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    print(\"PREDICTION RESULTS\")\n    print(\"=\"*60)\n    print(f\"\\nMost likely source: {label_names[pred_label]}\")\n    print(f\"Confidence: {all_probs[pred_label]*100:.2f}%\")\n    print(\"\\nAll probabilities:\")\n    print(\"-\"*60)\n    \n    sorted_indices = np.argsort(all_probs)[::-1]\n    for idx in sorted_indices:\n        bar_length = int(all_probs[idx] * 50)\n        bar = \"‚ñà\" * bar_length\n        print(f\"{label_names[idx]:20s} {all_probs[idx]*100:6.2f}% {bar}\")\n    print(\"=\"*60)\n    \n    return label_names[pred_label], all_probs[pred_label]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Test with Sample Text\n\nReplace the text below with any text you want to classify:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text - replace with your own!\n",
    "sample_text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "3. Finally, let's look at practical applications\n",
    "\n",
    "I hope this helps clarify things for you!\n",
    "\"\"\"\n",
    "\n",
    "predict_text(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Classify Your Own Text (Interactive)\n\nPaste your text in the input box below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive input\n",
    "print(\"Paste your text below and press Enter:\")\n",
    "user_text = input()\n",
    "\n",
    "if user_text.strip():\n",
    "    predict_text(model, user_text)\n",
    "else:\n",
    "    print(\"No text provided!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Batch Classification (Multiple Texts)\n\nYou can test multiple texts at once:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple texts\n",
    "texts_to_test = [\n",
    "    \"Sure, I can help with that!\",\n",
    "    \"I'd be happy to assist you with this question.\",\n",
    "    \"Let me break this down for you step by step.\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(texts_to_test, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEXT #{i}: {text[:50]}...\")\n",
    "    predict_text(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 9: Interactive Gradio UI\n\nLaunch an interactive web interface to classify text:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q gradio plotly\n\nimport gradio as gr\nimport plotly.graph_objects as go\nimport io\n\ndef predict_gradio(text):\n    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n    if not text.strip():\n        return \"Enter text to analyze\", None, \"‚ö†Ô∏è No text provided\"\n    \n    log_capture = io.StringIO()\n    \n    try:\n        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n        \n        log_capture.write(\"üîÑ Starting prediction...\\n\")\n        log_capture.write(f\"üìù Text length: {len(text)} characters\\n\")\n        \n        log_capture.write(\"\\nüî§ Tokenizing input...\\n\")\n        prepared_text = model.prepare_for_tokenization(text)\n        inputs = model.tokenize([prepared_text])\n        log_capture.write(\"‚úì Tokenization complete\\n\")\n        \n        if torch.cuda.is_available():\n            target_device = torch.device(\"cuda:0\")\n            log_capture.write(\"\\nüñ•Ô∏è  Device: GPU (cuda:0)\\n\")\n        else:\n            target_device = next(model.parameters()).device\n            log_capture.write(f\"\\nüñ•Ô∏è  Device: {target_device}\\n\")\n        \n        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n        \n        log_capture.write(\"\\nüß† Running model inference...\\n\")\n        with torch.no_grad():\n            embeddings = model.forward(inputs)\n            log_capture.write(f\"‚úì Generated embeddings: {embeddings.shape}\\n\")\n            \n            if hasattr(model, 'head'):\n                head_device = next(model.head.parameters()).device\n                embeddings = embeddings.to(head_device)\n            \n            embeddings = embeddings.to(torch.bfloat16)\n            logits = model.head(embeddings)\n            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n            log_capture.write(\"‚úì Computed probabilities\\n\")\n        \n        pred_label = torch.argmax(probabilities, dim=-1).item()\n        all_probs = probabilities[0].float().cpu().numpy()\n        \n        log_capture.write(f\"\\n{'='*40}\\n\")\n        log_capture.write(f\"üéØ Prediction: {label_names[pred_label]}\\n\")\n        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.1f}%\\n\")\n        log_capture.write(f\"{'='*40}\\n\\n\")\n        \n        sorted_indices = np.argsort(all_probs)[::-1]\n        log_capture.write(\"üìä All probabilities:\\n\")\n        for idx in sorted_indices:\n            bar = \"‚ñà\" * int(all_probs[idx] * 30)\n            log_capture.write(f\"  {label_names[idx]:12} {all_probs[idx]*100:5.1f}% {bar}\\n\")\n        \n        log_capture.write(\"\\n‚úÖ Analysis complete!\\n\")\n        \n        # Result text with clear formatting\n        result_text = f\"## Detected LLM: **{label_names[pred_label]}**\\n\\n### Confidence: **{all_probs[pred_label]*100:.1f}%**\"\n        \n        # Bar chart\n        sorted_labels = [label_names[i] for i in sorted_indices]\n        sorted_probs = [float(all_probs[i]) for i in sorted_indices]\n        \n        colors = ['#1f77b4' if i == 0 else '#aec7e8' for i in range(len(sorted_labels))]\n        \n        fig = go.Figure(data=[\n            go.Bar(\n                x=sorted_labels,\n                y=sorted_probs,\n                text=[f'{p*100:.1f}%' for p in sorted_probs],\n                textposition='outside',\n                marker_color=colors,\n                marker_line_width=0,\n            )\n        ])\n        \n        fig.update_layout(\n            xaxis_title=None,\n            yaxis_title=None,\n            yaxis=dict(range=[0, max(sorted_probs) * 1.15], showticklabels=False, showgrid=False),\n            xaxis=dict(showgrid=False),\n            height=200,\n            margin=dict(l=10, r=10, t=10, b=30),\n            showlegend=False,\n            plot_bgcolor='white',\n            paper_bgcolor='white',\n        )\n        \n        return result_text, fig, log_capture.getvalue()\n        \n    except Exception as e:\n        import traceback\n        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n        log_capture.write(error_msg)\n        \n        empty_fig = go.Figure()\n        empty_fig.update_layout(height=200)\n        return f\"Error: {str(e)}\", empty_fig, log_capture.getvalue()\n\n\n# Single viewport UI with logs\nwith gr.Blocks(title=\"Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\") as demo:\n    gr.Markdown(\"# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\")\n    gr.Markdown(\"**[Research Paper](https://eric-mingjie.github.io/llm-idiosyncrasies/index.html)** (97% accuracy) ‚Ä¢ **[GitHub](https://github.com/syedamaann/llm-idiosyncrasies)** ‚Ä¢ **[syedamaan.com](https://syedamaan.com)**\")\n    \n    with gr.Row():\n        # Left: Input\n        with gr.Column(scale=1):\n            text_input = gr.Textbox(\n                label=\"Input Text\",\n                placeholder=\"Paste text here...\",\n                lines=8,\n                max_lines=8,\n            )\n            submit_btn = gr.Button(\"Analyze\", variant=\"primary\", size=\"lg\")\n        \n        # Right: Results and Chart\n        with gr.Column(scale=1):\n            result_output = gr.Markdown(value=\"**Results will appear here**\")\n            plot_output = gr.Plot()\n    \n    # Bottom: Processing logs (compact)\n    logs_output = gr.Textbox(\n        label=\"Processing Log\",\n        lines=8,\n        max_lines=8,\n        interactive=False,\n        show_copy_button=True,\n    )\n    \n    submit_btn.click(\n        fn=predict_gradio,\n        inputs=text_input,\n        outputs=[result_output, plot_output, logs_output]\n    )\n    \n    text_input.submit(\n        fn=predict_gradio,\n        inputs=text_input,\n        outputs=[result_output, plot_output, logs_output]\n    )\n\ndemo.launch(share=True, debug=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}