{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier (Low Bandwidth Mode)\n",
    "\n",
    "Lightweight version optimized for low disk usage (downloads only 42KB head, streams model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n%pip install -q transformers>=4.56.2 peft==0.13.2 huggingface-hub accelerate\n%pip install -q llm2vec==0.2.3\n\nprint(\"‚úì Dependencies installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass(\"Enter your HuggingFace token: \")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download classification head only (Low Bandwidth mode)\n",
    "import os\n",
    "\n",
    "!rm -rf ./classifier_chat\n",
    "print(\"Downloading classification head (42 KB)...\")\n",
    "# Only downloads the small head file; base model is fetched via from_pretrained\n",
    "!huggingface-cli download Yida/classifier_chat head.pt --local-dir ./classifier_chat\n",
    "print(\"‚úì Download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from llm2vec import LLM2Vec\n",
    "import gc\n",
    "import os\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "\n",
    "def load_classifier_low_bandwidth(checkpoint_path=\"./classifier_chat\", num_labels=5):\n",
    "    print(\"Loading classifier (Low Bandwidth)...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    base_model_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    max_memory = {0: \"14GiB\", \"cpu\": \"30GiB\"}\n",
    "\n",
    "    # 1. Load Base Model\n",
    "    print(\"Loading base model...\")\n",
    "    config = AutoConfig.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_id,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": 0},\n",
    "        max_memory=max_memory,\n",
    "        offload_folder=\"./offload\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    # 2. Merge MNTP Adapter\n",
    "    print(\"Merging MNTP adapter...\")\n",
    "    model = PeftModel.from_pretrained(model, base_model_id, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    # 3. Load Supervised Adapter\n",
    "    print(\"Loading supervised adapter...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        f\"{base_model_id}-supervised\",\n",
    "        is_trainable=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # 4. LLM2Vec & Head\n",
    "    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
    "    hidden_size = list(model.modules())[-1].weight.shape[0]\n",
    "    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n",
    "\n",
    "    # Load Head Weights\n",
    "    head_file = os.path.join(checkpoint_path, \"head.pt\")\n",
    "    try:\n",
    "        target_device = next(model.parameters()).device\n",
    "    except:\n",
    "        target_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.head.load_state_dict(torch.load(head_file, map_location=target_device))\n",
    "    model.head = model.head.to(target_device)\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"‚úì Model loaded on GPU\" if torch.cuda.is_available() else \"‚úì Model loaded on CPU\")\n",
    "    return model\n",
    "\n",
    "model = load_classifier_low_bandwidth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, text):\n",
    "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "    \n",
    "    # Prepare & Tokenize\n",
    "    prepared_text = model.prepare_for_tokenization(text)\n",
    "    inputs = model.tokenize([prepared_text])\n",
    "    \n",
    "    # Device handling\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.forward(inputs)\n",
    "        if hasattr(model, 'head'):\n",
    "            embeddings = embeddings.to(next(model.head.parameters()).device)\n",
    "        \n",
    "        embeddings = embeddings.to(torch.bfloat16)\n",
    "        probs = torch.nn.functional.softmax(model.head(embeddings), dim=-1)\n",
    "    \n",
    "    # Results\n",
    "    pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "    all_probs = probs[0].float().cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nPrediction: {label_names[pred_idx]} ({all_probs[pred_idx]*100:.2f}%)\")\n",
    "    sorted_idxs = np.argsort(all_probs)[::-1]\n",
    "    for i in sorted_idxs:\n",
    "        print(f\"{label_names[i]:10} {all_probs[i]*100:6.2f}% {'‚ñà' * int(all_probs[i]*20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q gradio plotly\n",
    "\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go\n",
    "import io\n",
    "\n",
    "def predict_gradio(text):\n",
    "    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Enter text to analyze\", None, \"‚ö†Ô∏è No text provided\"\n",
    "    \n",
    "    log_capture = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "        \n",
    "        log_capture.write(\"üîÑ Starting prediction...\\n\")\n",
    "        log_capture.write(f\"üìù Text length: {len(text)} characters\\n\")\n",
    "        \n",
    "        log_capture.write(\"\\nüî§ Tokenizing input...\\n\")\n",
    "        prepared_text = model.prepare_for_tokenization(text)\n",
    "        inputs = model.tokenize([prepared_text])\n",
    "        log_capture.write(\"‚úì Tokenization complete\\n\")\n",
    "        \n",
    "        # Dynamic device detection\n",
    "        try:\n",
    "            target_device = next(model.parameters()).device\n",
    "        except:\n",
    "            target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        log_capture.write(f\"\\nüñ•Ô∏è  Device: {target_device}\\n\")\n",
    "        \n",
    "        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        \n",
    "        log_capture.write(\"\\nüß† Running model inference...\\n\")\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.forward(inputs)\n",
    "            log_capture.write(f\"‚úì Generated embeddings: {embeddings.shape}\\n\")\n",
    "            \n",
    "            if hasattr(model, 'head'):\n",
    "                head_device = next(model.head.parameters()).device\n",
    "                embeddings = embeddings.to(head_device)\n",
    "            \n",
    "            embeddings = embeddings.to(torch.bfloat16)\n",
    "            logits = model.head(embeddings)\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            log_capture.write(\"‚úì Computed probabilities\\n\")\n",
    "        \n",
    "        pred_label = torch.argmax(probabilities, dim=-1).item()\n",
    "        all_probs = probabilities[0].float().cpu().numpy()\n",
    "        \n",
    "        log_capture.write(f\"\\n{'='*40}\\n\")\n",
    "        log_capture.write(f\"üéØ Prediction: {label_names[pred_label]}\\n\")\n",
    "        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.1f}%\\n\")\n",
    "        log_capture.write(f\"{'='*40}\\n\\n\")\n",
    "        \n",
    "        sorted_indices = np.argsort(all_probs)[::-1]\n",
    "        log_capture.write(\"üìä All probabilities:\\n\")\n",
    "        for idx in sorted_indices:\n",
    "            bar = \"‚ñà\" * int(all_probs[idx] * 30)\n",
    "            log_capture.write(f\"  {label_names[idx]:12} {all_probs[idx]*100:5.1f}% {bar}\\n\")\n",
    "        \n",
    "        log_capture.write(\"\\n‚úÖ Analysis complete!\\n\")\n",
    "        \n",
    "        # Result text with clear formatting\n",
    "        result_text = f\"## Detected LLM: **{label_names[pred_label]}**\\n\\n### Confidence: **{all_probs[pred_label]*100:.1f}%**\"\n",
    "        \n",
    "        # Bar chart\n",
    "        sorted_labels = [label_names[i] for i in sorted_indices]\n",
    "        sorted_probs = [float(all_probs[i]) for i in sorted_indices]\n",
    "        \n",
    "        colors = ['#1f77b4' if i == 0 else '#aec7e8' for i in range(len(sorted_labels))]\n",
    "        \n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=sorted_labels,\n",
    "                y=sorted_probs,\n",
    "                text=[f'{p*100:.1f}%' for p in sorted_probs],\n",
    "                textposition='outside',\n",
    "                marker_color=colors,\n",
    "                marker_line_width=0,\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title=None,\n",
    "            yaxis_title=None,\n",
    "            yaxis=dict(range=[0, max(sorted_probs) * 1.15], showticklabels=False, showgrid=False),\n",
    "            xaxis=dict(showgrid=False),\n",
    "            height=200,\n",
    "            margin=dict(l=10, r=10, t=10, b=30),\n",
    "            showlegend=False,\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "        )\n",
    "        \n",
    "        return result_text, fig, log_capture.getvalue()\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
    "        log_capture.write(error_msg)\n",
    "        \n",
    "        empty_fig = go.Figure()\n",
    "        empty_fig.update_layout(height=200)\n",
    "        return f\"Error: {str(e)}\", empty_fig, log_capture.getvalue()\n",
    "\n",
    "\n",
    "# Single viewport UI with logs\n",
    "with gr.Blocks(title=\"Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\") as demo:\n",
    "    gr.Markdown(\"# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\")\n",
    "    gr.Markdown(\"**[Research Paper](https://eric-mingjie.github.io/llm-idiosyncrasies/index.html)** (97% accuracy) ‚Ä¢ **[GitHub](https://github.com/syedamaann/llm-idiosyncrasies)** ‚Ä¢ **[syedamaan.com](https://syedamaan.com)**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left: Input\n",
    "        with gr.Column(scale=1):\n",
    "            text_input = gr.Textbox(\n",
    "                label=\"Input Text\",\n",
    "                placeholder=\"Paste text here...\",\n",
    "                lines=8,\n",
    "                max_lines=8,\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\", size=\"lg\")\n",
    "        \n",
    "        # Right: Results and Chart\n",
    "        with gr.Column(scale=1):\n",
    "            result_output = gr.Markdown(value=\"**Results will appear here**\")\n",
    "            plot_output = gr.Plot()\n",
    "    \n",
    "    # Bottom: Processing logs (compact)\n",
    "    logs_output = gr.Textbox(\n",
    "        label=\"Processing Log\",\n",
    "        lines=8,\n",
    "        max_lines=8,\n",
    "        interactive=False,\n",
    "        show_copy_button=True,\n",
    "    )\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=predict_gradio,\n",
    "        inputs=text_input,\n",
    "        outputs=[result_output, plot_output, logs_output]\n",
    "    )\n",
    "    \n",
    "    text_input.submit(\n",
    "        fn=predict_gradio,\n",
    "        inputs=text_input,\n",
    "        outputs=[result_output, plot_output, logs_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "\"\"\"\n",
    "predict_text(model, text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}