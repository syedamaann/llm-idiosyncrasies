{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier (Low Bandwidth Mode)\n",
    "\n",
    "Lightweight version optimized for low disk usage (downloads only 42KB head, streams model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì Dependencies installed\n",
      "‚ö†Ô∏è  IMPORTANT: Restart the kernel before continuing!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q transformers==4.44.2 peft==0.13.2 huggingface-hub accelerate safetensors\n",
    "%pip install -q llm2vec==0.2.3 gradio plotly hf_transfer\n",
    "\n",
    "print(\"‚úì Dependencies installed\")\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Restart the kernel before continuing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.44.2\n",
      "‚úì Version check passed\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "assert transformers.__version__ == \"4.44.2\", f\"Need transformers==4.44.2 for llm2vec compatibility, got {transformers.__version__}\"\n",
    "print(\"‚úì Version check passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Classification Head\n",
    "\n",
    "This notebook only downloads the classification head (42KB) and streams the base model, saving significant disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading classification head (42 KB)...\n",
      "\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Downloading 'head.pt' to 'classifier_chat/.cache/huggingface/download/YRa3C5umg44TBDeunKxqMu5V1K4=.c0858f64786ece9aee3b49091464a82fb78981c292dd87775d1d004a5ece5795.incomplete'\n",
      "head.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.4k/42.4k [00:00<00:00, 110kB/s]\n",
      "Download complete. Moving file to classifier_chat/head.pt\n",
      "classifier_chat/head.pt\n",
      "‚úì Download complete\n"
     ]
    }
   ],
   "source": [
    "# Download classification head only (Low Bandwidth mode)\n",
    "# CRITICAL: Use /workspace for persistent storage\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = \"/workspace/classifier_chat\"\n",
    "OFFLOAD_PATH = \"/workspace/offload\"\n",
    "\n",
    "!rm -rf {CHECKPOINT_PATH}\n",
    "os.makedirs(OFFLOAD_PATH, exist_ok=True)\n",
    "print(\"Downloading classification head (42 KB)...\")\n",
    "!huggingface-cli download Yida/classifier_chat head.pt --local-dir {CHECKPOINT_PATH}\n",
    "print(f\"‚úì Download complete to {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Clearing HuggingFace cache...\n",
      "  Deleting: /workspace/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
      "  ‚úì Cleared: /workspace/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
      "  Deleting: /workspace/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
      "  ‚úì Cleared: /workspace/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
      "\n",
      "‚úÖ Cache cleared! OLD model code will be downloaded on next load.\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: Run this cell, THEN immediately run the model loading cell!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# CRITICAL: Clear ALL HuggingFace cache for this model\n",
    "cache_locations = [\n",
    "    \"/workspace/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    os.path.expanduser(\"~/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
    "    \"/workspace/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
    "    os.path.expanduser(\"~/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
    "]\n",
    "\n",
    "print(\"üóëÔ∏è  Clearing HuggingFace cache...\")\n",
    "cleared_any = False\n",
    "for cache_dir in cache_locations:\n",
    "    if os.path.exists(cache_dir):\n",
    "        print(f\"  Deleting: {cache_dir}\")\n",
    "        try:\n",
    "            shutil.rmtree(cache_dir)\n",
    "            cleared_any = True\n",
    "            print(f\"  ‚úì Cleared: {cache_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Failed to clear {cache_dir}: {e}\")\n",
    "\n",
    "if cleared_any:\n",
    "    print(\"\\n‚úÖ Cache cleared! OLD model code will be downloaded on next load.\")\n",
    "else:\n",
    "    print(\"\\n‚úì No cache found. OLD model code will be downloaded on first load.\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Run this cell, THEN immediately run the model loading cell!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classifier (Low Bandwidth)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading tokenizer...\n",
      "‚úì Tokenizer loaded\n",
      "Step 2: Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:01<00:00, 15.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Base model loaded\n",
      "Step 3: Merging MNTP adapter...\n",
      "‚úì MNTP adapter merged\n",
      "Step 4: Loading supervised adapter...\n",
      "‚úì Supervised adapter loaded\n",
      "Step 5: Wrapping with LLM2Vec...\n",
      "Step 6: Adding classification head...\n",
      "‚úì Classification head loaded\n",
      "\n",
      "‚úÖ Model fully loaded on GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from llm2vec import LLM2Vec\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# CRITICAL: Use persistent volume paths\n",
    "CHECKPOINT_PATH = \"/workspace/classifier_chat\"\n",
    "OFFLOAD_PATH = \"/workspace/offload\"\n",
    "\n",
    "def load_classifier_low_bandwidth(checkpoint_path=CHECKPOINT_PATH, num_labels=5):\n",
    "    \"\"\"\n",
    "    Memory-optimized model loading for low bandwidth environments.\n",
    "    Uses /workspace for all large files (persistent volume).\n",
    "    \"\"\"\n",
    "    print(\"Loading classifier (Low Bandwidth)...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    mntp_adapter_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
    "    supervised_adapter_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\"\n",
    "\n",
    "    # Ensure offload directory exists\n",
    "    os.makedirs(OFFLOAD_PATH, exist_ok=True)\n",
    "    \n",
    "    # Load tokenizer with authentication\n",
    "    print(\"Step 1: Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_id,\n",
    "        trust_remote_code=True,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Set padding for LLM2Vec compatibility\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"left\"  # Required by LLM2Vec pooling\n",
    "    \n",
    "    print(f\"‚úì Tokenizer loaded (padding_side={tokenizer.padding_side})\")\n",
    "\n",
    "    # Load base model with authentication\n",
    "    print(\"Step 2: Loading base model...\")\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        base_model_id,\n",
    "        trust_remote_code=True,\n",
    "        token=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",\n",
    "        offload_folder=OFFLOAD_PATH,  # Use persistent volume\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        token=True\n",
    "    )\n",
    "    print(\"‚úì Base model loaded\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Merge MNTP Adapter\n",
    "    print(\"Step 3: Merging MNTP adapter...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        mntp_adapter_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        token=True\n",
    "    )\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"‚úì MNTP adapter merged\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load Supervised Adapter\n",
    "    print(\"Step 4: Loading supervised adapter...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        supervised_adapter_id,\n",
    "        is_trainable=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        token=True\n",
    "    )\n",
    "    print(\"‚úì Supervised adapter loaded\")\n",
    "\n",
    "    # CRITICAL: Re-confirm tokenizer padding_side before LLM2Vec wrapper\n",
    "    # (PEFT operations might interfere with tokenizer state)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Wrap with LLM2Vec\n",
    "    print(\"Step 5: Wrapping with LLM2Vec...\")\n",
    "    model = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
    "    \n",
    "    # Verify tokenizer padding_side in LLM2Vec wrapper\n",
    "    assert model.tokenizer.padding_side == \"left\", f\"Tokenizer padding_side must be 'left', got '{model.tokenizer.padding_side}'\"\n",
    "    \n",
    "    print(\"Step 6: Adding classification head...\")\n",
    "    hidden_size = config.hidden_size\n",
    "    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n",
    "\n",
    "    # Load Head Weights\n",
    "    head_file = os.path.join(checkpoint_path, \"head.pt\")\n",
    "    if os.path.exists(head_file):\n",
    "        target_device = torch.device(\"cuda:0\")\n",
    "        model.head.load_state_dict(torch.load(head_file, map_location=target_device, weights_only=True))\n",
    "        model.head = model.head.to(target_device)\n",
    "        print(\"‚úì Classification head loaded\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Head not found at {head_file}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model fully loaded on GPU!\")\n",
    "    print(f\"   VRAM used: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
    "    return model\n",
    "\n",
    "model = load_classifier_low_bandwidth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(model, text):\n",
    "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "    \n",
    "    # Prepare & Tokenize\n",
    "    prepared_text = model.prepare_for_tokenization(text)\n",
    "    inputs = model.tokenize([prepared_text])\n",
    "    \n",
    "    # Device handling\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.forward(inputs)\n",
    "        if hasattr(model, 'head'):\n",
    "            embeddings = embeddings.to(next(model.head.parameters()).device)\n",
    "        \n",
    "        embeddings = embeddings.to(torch.bfloat16)\n",
    "        probs = torch.nn.functional.softmax(model.head(embeddings), dim=-1)\n",
    "    \n",
    "    # Results\n",
    "    pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "    all_probs = probs[0].float().cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nPrediction: {label_names[pred_idx]} ({all_probs[pred_idx]*100:.2f}%)\")\n",
    "    sorted_idxs = np.argsort(all_probs)[::-1]\n",
    "    for i in sorted_idxs:\n",
    "        print(f\"{label_names[i]:10} {all_probs[i]*100:6.2f}% {'‚ñà' * int(all_probs[i]*20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Pooling modes are implemented for padding from left.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mHello! I\u001b[39m\u001b[33m'\u001b[39m\u001b[33md be happy to help you with that question. Let me break this down into a few key points:\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m1. First, it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms important to understand the context\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m2. Second, we should consider the implications\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mpredict_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mpredict_text\u001b[39m\u001b[34m(model, text)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mhead\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     20\u001b[39m         embeddings = embeddings.to(\u001b[38;5;28mnext\u001b[39m(model.head.parameters()).device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/llm2vec/llm2vec.py:239\u001b[39m, in \u001b[36mLLM2Vec.forward\u001b[39m\u001b[34m(self, sentence_feature)\u001b[39m\n\u001b[32m    236\u001b[39m reps = \u001b[38;5;28mself\u001b[39m.model(**sentence_feature)\n\u001b[32m    237\u001b[39m sentence_feature[\u001b[33m\"\u001b[39m\u001b[33membed_mask\u001b[39m\u001b[33m\"\u001b[39m] = embed_mask\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/llm2vec/llm2vec.py:243\u001b[39m, in \u001b[36mLLM2Vec.get_pooling\u001b[39m\u001b[34m(self, features, last_hidden_states)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_pooling\u001b[39m(\u001b[38;5;28mself\u001b[39m, features, last_hidden_states):  \u001b[38;5;66;03m# All models padded from left\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m         \u001b[38;5;28mself\u001b[39m.tokenizer.padding_side == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    244\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mPooling modes are implemented for padding from left.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_instruction:\n\u001b[32m    246\u001b[39m         \u001b[38;5;28mself\u001b[39m._skip_instruction(features)\n",
      "\u001b[31mAssertionError\u001b[39m: Pooling modes are implemented for padding from left."
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
    "1. First, it's important to understand the context\n",
    "2. Second, we should consider the implications\n",
    "\"\"\"\n",
    "predict_text(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "Textbox.__init__() got an unexpected keyword argument 'show_copy_button'",
       "name": "TypeError",
       "stack": "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    128\u001b[39m         plot_output = gr.Plot()\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Bottom: Processing logs (compact)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m logs_output = \u001b[43mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing Log\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43minteractive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_copy_button\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m submit_btn.click(\n\u001b[32m    140\u001b[39m     fn=predict_gradio,\n\u001b[32m    141\u001b[39m     inputs=text_input,\n\u001b[32m    142\u001b[39m     outputs=[result_output, plot_output, logs_output]\n\u001b[32m    143\u001b[39m )\n\u001b[32m    145\u001b[39m text_input.submit(\n\u001b[32m    146\u001b[39m     fn=predict_gradio,\n\u001b[32m    147\u001b[39m     inputs=text_input,\n\u001b[32m    148\u001b[39m     outputs=[result_output, plot_output, logs_output]\n\u001b[32m    149\u001b[39m )\n\n\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py:194\u001b[39m, in \u001b[36mupdateable.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\n\u001b[31mTypeError\u001b[39m: Textbox.__init__() got an unexpected keyword argument 'show_copy_button'"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://161d4b34ab1c618f62.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q gradio plotly\n",
    "\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go\n",
    "import io\n",
    "\n",
    "def predict_gradio(text):\n",
    "    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Enter text to analyze\", None, \"‚ö†Ô∏è No text provided\"\n",
    "    \n",
    "    log_capture = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
    "        \n",
    "        log_capture.write(\"üîÑ Starting prediction...\\n\")\n",
    "        log_capture.write(f\"üìù Text length: {len(text)} characters\\n\")\n",
    "        \n",
    "        log_capture.write(\"\\nüî§ Tokenizing input...\\n\")\n",
    "        prepared_text = model.prepare_for_tokenization(text)\n",
    "        inputs = model.tokenize([prepared_text])\n",
    "        log_capture.write(\"‚úì Tokenization complete\\n\")\n",
    "        \n",
    "        # Dynamic device detection\n",
    "        try:\n",
    "            target_device = next(model.parameters()).device\n",
    "        except:\n",
    "            target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        log_capture.write(f\"\\nüñ•Ô∏è  Device: {target_device}\\n\")\n",
    "        \n",
    "        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "        \n",
    "        log_capture.write(\"\\nüß† Running model inference...\\n\")\n",
    "        with torch.no_grad():\n",
    "            embeddings = model.forward(inputs)\n",
    "            log_capture.write(f\"‚úì Generated embeddings: {embeddings.shape}\\n\")\n",
    "            \n",
    "            if hasattr(model, 'head'):\n",
    "                head_device = next(model.head.parameters()).device\n",
    "                embeddings = embeddings.to(head_device)\n",
    "            \n",
    "            embeddings = embeddings.to(torch.bfloat16)\n",
    "            logits = model.head(embeddings)\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            log_capture.write(\"‚úì Computed probabilities\\n\")\n",
    "        \n",
    "        pred_label = torch.argmax(probabilities, dim=-1).item()\n",
    "        all_probs = probabilities[0].float().cpu().numpy()\n",
    "        \n",
    "        log_capture.write(f\"\\n{'='*40}\\n\")\n",
    "        log_capture.write(f\"üéØ Prediction: {label_names[pred_label]}\\n\")\n",
    "        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.1f}%\\n\")\n",
    "        log_capture.write(f\"{'='*40}\\n\\n\")\n",
    "        \n",
    "        sorted_indices = np.argsort(all_probs)[::-1]\n",
    "        log_capture.write(\"üìä All probabilities:\\n\")\n",
    "        for idx in sorted_indices:\n",
    "            bar = \"‚ñà\" * int(all_probs[idx] * 30)\n",
    "            log_capture.write(f\"  {label_names[idx]:12} {all_probs[idx]*100:5.1f}% {bar}\\n\")\n",
    "        \n",
    "        log_capture.write(\"\\n‚úÖ Analysis complete!\\n\")\n",
    "        \n",
    "        # Result text with clear formatting\n",
    "        result_text = f\"## Detected LLM: **{label_names[pred_label]}**\\n\\n### Confidence: **{all_probs[pred_label]*100:.1f}%**\"\n",
    "        \n",
    "        # Bar chart\n",
    "        sorted_labels = [label_names[i] for i in sorted_indices]\n",
    "        sorted_probs = [float(all_probs[i]) for i in sorted_indices]\n",
    "        \n",
    "        colors = ['#1f77b4' if i == 0 else '#aec7e8' for i in range(len(sorted_labels))]\n",
    "        \n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=sorted_labels,\n",
    "                y=sorted_probs,\n",
    "                text=[f'{p*100:.1f}%' for p in sorted_probs],\n",
    "                textposition='outside',\n",
    "                marker_color=colors,\n",
    "                marker_line_width=0,\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title=None,\n",
    "            yaxis_title=None,\n",
    "            yaxis=dict(range=[0, max(sorted_probs) * 1.15], showticklabels=False, showgrid=False),\n",
    "            xaxis=dict(showgrid=False),\n",
    "            height=200,\n",
    "            margin=dict(l=10, r=10, t=10, b=30),\n",
    "            showlegend=False,\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "        )\n",
    "        \n",
    "        return result_text, fig, log_capture.getvalue()\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
    "        log_capture.write(error_msg)\n",
    "        \n",
    "        empty_fig = go.Figure()\n",
    "        empty_fig.update_layout(height=200)\n",
    "        return f\"Error: {str(e)}\", empty_fig, log_capture.getvalue()\n",
    "\n",
    "\n",
    "# Single viewport UI with logs\n",
    "with gr.Blocks(title=\"Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\") as demo:\n",
    "    gr.Markdown(\"# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\")\n",
    "    gr.Markdown(\"**[Research Paper](https://eric-mingjie.github.io/llm-idiosyncrasies/index.html)** (97% accuracy) ‚Ä¢ **[GitHub](https://github.com/syedamaann/llm-idiosyncrasies)** ‚Ä¢ **[syedamaan.com](https://syedamaan.com)**\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left: Input\n",
    "        with gr.Column(scale=1):\n",
    "            text_input = gr.Textbox(\n",
    "                label=\"Input Text\",\n",
    "                placeholder=\"Paste text here...\",\n",
    "                lines=8,\n",
    "                max_lines=8,\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\", size=\"lg\")\n",
    "        \n",
    "        # Right: Results and Chart\n",
    "        with gr.Column(scale=1):\n",
    "            result_output = gr.Markdown(value=\"**Results will appear here**\")\n",
    "            plot_output = gr.Plot()\n",
    "    \n",
    "    # Bottom: Processing logs (compact)\n",
    "    logs_output = gr.Textbox(\n",
    "        label=\"Processing Log\",\n",
    "        lines=8,\n",
    "        max_lines=8,\n",
    "        interactive=False,\n",
    "    )\n",
    "    \n",
    "    submit_btn.click(\n",
    "        fn=predict_gradio,\n",
    "        inputs=text_input,\n",
    "        outputs=[result_output, plot_output, logs_output]\n",
    "    )\n",
    "    \n",
    "    text_input.submit(\n",
    "        fn=predict_gradio,\n",
    "        inputs=text_input,\n",
    "        outputs=[result_output, plot_output, logs_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
