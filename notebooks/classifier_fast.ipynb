{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classifier\n",
        "\n",
        "Lightweight version using pre-merged weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "‚úì Dependencies installed\n",
            "‚ö†Ô∏è  IMPORTANT: Restart the kernel before continuing!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "%pip install --upgrade pip\n",
        "%pip install -q transformers==4.44.2 peft==0.13.2 huggingface-hub accelerate safetensors\n",
        "%pip install -q llm2vec==0.2.3 gradio plotly hf_transfer\n",
        "\n",
        "print(\"‚úì Dependencies installed\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Restart the kernel before continuing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.44.2\n",
            "‚úì Version check passed\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "assert transformers.__version__ == \"4.44.2\", f\"Need transformers==4.44.2 for llm2vec compatibility, got {transformers.__version__}\"\n",
        "print(\"‚úì Version check passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model (15.5 GB) to /workspace/classifier_chat...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 8 files:   0%|                                   | 0/8 [00:00<?, ?it/s]Still waiting to acquire lock on /workspace/classifier_chat/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
            "Still waiting to acquire lock on /workspace/classifier_chat/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
            "Still waiting to acquire lock on /workspace/classifier_chat/.cache/huggingface/.gitignore.lock (elapsed: 0.1 seconds)\n",
            "Downloading 'scheduler.pt' to '/workspace/classifier_chat/.cache/huggingface/download/-TKHT9st1Ll35ofujHvZcGIOiAc=.505e140348cf428a705b42cb8aafd791b8d0b0ac55c70e82b985098cfa08a46a.incomplete'\n",
            "Downloading 'training_args.bin' to '/workspace/classifier_chat/.cache/huggingface/download/wDrERwdtvdWRGcZ53Ku-FoERHzQ=.e655c9f4ec242123684bdefde757c27947012689a113dea3c6edf3c217226334.incomplete'\n",
            "\n",
            "scheduler.pt:   0%|                                 | 0.00/1.06k [00:00<?, ?B/s]\u001b[ADownloading 'model.safetensors' to '/workspace/classifier_chat/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.3d21d1a3f2d59ff771ba0c63db81e3f625fb3dc1c4c0b3371cce64bb28ea4940.incomplete'\n",
            "Downloading 'optimizer.pt' to '/workspace/classifier_chat/.cache/huggingface/download/FLSH5hVUNNxQuAanKK0z708kvhA=.77e72f0b05410bbd4ca8ae7b4b2a128906dfcadef413f799e494094d36ab5f6c.incomplete'\n",
            "Downloading 'head.pt' to '/workspace/classifier_chat/.cache/huggingface/download/YRa3C5umg44TBDeunKxqMu5V1K4=.c0858f64786ece9aee3b49091464a82fb78981c292dd87775d1d004a5ece5795.incomplete'\n",
            "Downloading 'rng_state.pth' to '/workspace/classifier_chat/.cache/huggingface/download/-DG7bmjuTHp_EcDNQlnRqoCQTt4=.70baeded00dccc74bd1983a5270f31dfa41517005de6aadbf3fc2e13929870d8.incomplete'\n",
            "Downloading 'trainer_state.json' to '/workspace/classifier_chat/.cache/huggingface/download/1uiJ87Eu3vvlOlZKj0NZvGlSO3I=.c959132d2aba9de7c634b580352818e7fb91daf6.incomplete'\n",
            "Downloading '.gitattributes' to '/workspace/classifier_chat/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "\n",
            "\n",
            "trainer_state.json: 7.47kB [00:00, 12.1MB/s]A\n",
            "\n",
            "\n",
            ".gitattributes: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            ".gitattributes: 1.52kB [00:00, 587kB/s]             | 0.00/5.37k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/trainer_state.json\n",
            "Download complete. Moving file to /workspace/classifier_chat/.gitattributes\n",
            "Fetching 8 files:  12%|‚ñà‚ñà‚ñà‚ñç                       | 1/8 [00:00<00:03,  2.19it/s]\n",
            "\n",
            "model.safetensors:   0%|                            | 0.00/15.2G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "head.pt:   0%|                                      | 0.00/42.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth:   0%|                                | 0.00/14.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   0%|                                  | 0.00/336M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "scheduler.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.06k/1.06k [00:00<00:00, 1.42kB/s]\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/scheduler.pt\n",
            "\n",
            "\n",
            "\n",
            "training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.37k/5.37k [00:00<00:00, 7.84kB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/training_args.bin\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.2k/14.2k [00:00<00:00, 23.0kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/rng_state.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "head.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.4k/42.4k [00:00<00:00, 67.2kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/head.pt\n",
            "Fetching 8 files:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 2/8 [00:01<00:03,  1.62it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   0%|                          | 809k/336M [00:00<05:06, 1.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà                    | 67.8M/336M [00:01<00:04, 62.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   0%|                   | 34.0M/15.2G [00:01<12:56, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 135M/336M [00:01<00:01, 107MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 202M/336M [00:01<00:00, 141MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   1%|‚ñè                   | 101M/15.2G [00:02<04:55, 51.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   2%|‚ñç                    | 302M/15.2G [00:02<01:16, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 269M/336M [00:02<00:00, 124MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 336M/336M [00:03<00:00, 103MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/optimizer.pt\n",
            "\n",
            "\n",
            "model.safetensors:   3%|‚ñå                    | 436M/15.2G [00:03<01:37, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   5%|‚ñâ                    | 705M/15.2G [00:03<00:50, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   6%|‚ñà‚ñè                   | 839M/15.2G [00:04<00:46, 310MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   6%|‚ñà‚ñé                   | 906M/15.2G [00:04<00:43, 331MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   6%|‚ñà‚ñé                   | 973M/15.2G [00:04<00:44, 318MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   7%|‚ñà‚ñé                  | 1.04G/15.2G [00:04<00:45, 312MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   7%|‚ñà‚ñç                  | 1.11G/15.2G [00:04<00:44, 314MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   8%|‚ñà‚ñå                  | 1.17G/15.2G [00:05<00:40, 344MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   8%|‚ñà‚ñå                  | 1.22G/15.2G [00:05<00:49, 279MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   8%|‚ñà‚ñã                  | 1.29G/15.2G [00:05<00:44, 313MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   9%|‚ñà‚ñä                  | 1.36G/15.2G [00:05<00:39, 348MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:   9%|‚ñà‚ñâ                  | 1.42G/15.2G [00:05<00:40, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  10%|‚ñà‚ñà                  | 1.56G/15.2G [00:06<00:28, 482MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  11%|‚ñà‚ñà‚ñè                 | 1.62G/15.2G [00:06<00:31, 430MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  11%|‚ñà‚ñà‚ñè                 | 1.69G/15.2G [00:06<00:32, 413MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  12%|‚ñà‚ñà‚ñé                 | 1.76G/15.2G [00:07<01:01, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  12%|‚ñà‚ñà‚ñç                 | 1.89G/15.2G [00:07<00:38, 342MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  13%|‚ñà‚ñà‚ñã                 | 2.00G/15.2G [00:07<00:44, 298MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  14%|‚ñà‚ñà‚ñã                 | 2.07G/15.2G [00:07<00:40, 323MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  14%|‚ñà‚ñà‚ñä                 | 2.17G/15.2G [00:08<00:39, 332MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  15%|‚ñà‚ñà‚ñà                 | 2.31G/15.2G [00:08<00:29, 431MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  16%|‚ñà‚ñà‚ñà‚ñè                | 2.37G/15.2G [00:08<00:32, 397MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  16%|‚ñà‚ñà‚ñà‚ñè                | 2.44G/15.2G [00:08<00:33, 375MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17%|‚ñà‚ñà‚ñà‚ñé                | 2.51G/15.2G [00:08<00:34, 368MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17%|‚ñà‚ñà‚ñà‚ñç                | 2.57G/15.2G [00:09<00:32, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  17%|‚ñà‚ñà‚ñà‚ñç                | 2.64G/15.2G [00:09<00:33, 372MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  18%|‚ñà‚ñà‚ñà‚ñå                | 2.71G/15.2G [00:09<00:41, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  18%|‚ñà‚ñà‚ñà‚ñã                | 2.75G/15.2G [00:09<00:49, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  19%|‚ñà‚ñà‚ñà‚ñã                | 2.82G/15.2G [00:10<00:42, 290MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  19%|‚ñà‚ñà‚ñà‚ñâ                | 2.95G/15.2G [00:10<00:32, 380MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  20%|‚ñà‚ñà‚ñà‚ñâ                | 3.02G/15.2G [00:10<00:31, 389MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21%|‚ñà‚ñà‚ñà‚ñà‚ñè               | 3.15G/15.2G [00:10<00:24, 482MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  21%|‚ñà‚ñà‚ñà‚ñà‚ñè               | 3.22G/15.2G [00:10<00:28, 414MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  22%|‚ñà‚ñà‚ñà‚ñà‚ñé               | 3.29G/15.2G [00:11<00:30, 390MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  22%|‚ñà‚ñà‚ñà‚ñà‚ñç               | 3.36G/15.2G [00:11<00:33, 356MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23%|‚ñà‚ñà‚ñà‚ñà‚ñå               | 3.42G/15.2G [00:11<00:30, 381MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23%|‚ñà‚ñà‚ñà‚ñà‚ñå               | 3.49G/15.2G [00:11<00:33, 346MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  23%|‚ñà‚ñà‚ñà‚ñà‚ñã               | 3.56G/15.2G [00:11<00:30, 379MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  24%|‚ñà‚ñà‚ñà‚ñà‚ñä               | 3.62G/15.2G [00:12<00:32, 359MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  24%|‚ñà‚ñà‚ñà‚ñà‚ñä               | 3.69G/15.2G [00:12<00:30, 381MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  25%|‚ñà‚ñà‚ñà‚ñà‚ñâ               | 3.76G/15.2G [00:12<00:31, 365MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà               | 3.82G/15.2G [00:12<00:28, 394MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 3.92G/15.2G [00:12<00:29, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 3.98G/15.2G [00:12<00:28, 398MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé              | 4.03G/15.2G [00:13<00:32, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 4.09G/15.2G [00:13<00:29, 376MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 4.16G/15.2G [00:13<00:33, 326MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 4.23G/15.2G [00:13<00:30, 363MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 4.27G/15.2G [00:13<00:33, 322MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã              | 4.34G/15.2G [00:13<00:29, 364MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 4.40G/15.2G [00:14<00:33, 318MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ              | 4.54G/15.2G [00:14<00:22, 476MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 4.61G/15.2G [00:15<00:49, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 4.87G/15.2G [00:15<00:26, 394MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 4.94G/15.2G [00:15<00:25, 404MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 5.04G/15.2G [00:15<00:26, 376MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 5.10G/15.2G [00:16<00:40, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 5.24G/15.2G [00:16<00:31, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 5.37G/15.2G [00:16<00:25, 378MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 5.50G/15.2G [00:17<00:20, 467MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 5.57G/15.2G [00:17<00:22, 428MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 5.64G/15.2G [00:17<00:35, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 5.71G/15.2G [00:18<00:33, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 5.77G/15.2G [00:18<00:29, 319MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 5.84G/15.2G [00:18<00:28, 328MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 6.11G/15.2G [00:18<00:13, 657MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 6.24G/15.2G [00:18<00:16, 539MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 6.38G/15.2G [00:19<00:22, 388MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 6.44G/15.2G [00:19<00:24, 356MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 6.51G/15.2G [00:19<00:24, 348MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 6.64G/15.2G [00:20<00:18, 457MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 6.78G/15.2G [00:20<00:15, 542MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 6.91G/15.2G [00:20<00:24, 336MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 6.98G/15.2G [00:21<00:22, 357MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 7.05G/15.2G [00:21<00:23, 346MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 7.11G/15.2G [00:21<00:22, 366MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå          | 7.25G/15.2G [00:21<00:18, 426MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã          | 7.38G/15.2G [00:21<00:14, 521MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 7.45G/15.2G [00:22<00:17, 451MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 7.52G/15.2G [00:22<00:18, 410MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 7.58G/15.2G [00:22<00:22, 337MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 7.65G/15.2G [00:22<00:20, 361MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 7.72G/15.2G [00:22<00:19, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 7.78G/15.2G [00:23<00:19, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 7.89G/15.2G [00:23<00:16, 435MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 7.96G/15.2G [00:23<00:18, 386MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 8.03G/15.2G [00:23<00:19, 376MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 8.09G/15.2G [00:23<00:20, 349MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 8.16G/15.2G [00:24<00:18, 373MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä         | 8.23G/15.2G [00:24<00:17, 395MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 8.29G/15.2G [00:24<00:18, 375MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 8.36G/15.2G [00:24<00:19, 344MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 8.43G/15.2G [00:24<00:20, 325MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 8.49G/15.2G [00:25<00:18, 353MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 8.56G/15.2G [00:25<00:17, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 8.63G/15.2G [00:25<00:16, 400MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 8.70G/15.2G [00:25<00:17, 380MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 8.76G/15.2G [00:25<00:15, 405MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 8.83G/15.2G [00:26<00:28, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 8.90G/15.2G [00:26<00:23, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 8.96G/15.2G [00:26<00:23, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 9.10G/15.2G [00:26<00:15, 394MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 9.30G/15.2G [00:27<00:10, 568MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 9.37G/15.2G [00:27<00:12, 484MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 9.43G/15.2G [00:27<00:12, 450MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 9.50G/15.2G [00:27<00:13, 420MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 9.57G/15.2G [00:27<00:13, 407MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 9.63G/15.2G [00:28<00:15, 364MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 9.70G/15.2G [00:28<00:16, 337MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 9.77G/15.2G [00:28<00:15, 339MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 9.84G/15.2G [00:28<00:15, 344MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 9.90G/15.2G [00:28<00:14, 373MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 9.97G/15.2G [00:28<00:13, 398MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 10.0G/15.2G [00:29<00:14, 353MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 10.1G/15.2G [00:29<00:15, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 10.2G/15.2G [00:29<00:14, 352MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 10.2G/15.2G [00:29<00:13, 372MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 10.3G/15.2G [00:29<00:13, 369MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 10.4G/15.2G [00:30<00:12, 390MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 10.4G/15.2G [00:30<00:11, 396MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 10.5G/15.2G [00:30<00:11, 419MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 10.6G/15.2G [00:30<00:18, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 10.7G/15.2G [00:31<00:13, 338MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 10.8G/15.2G [00:31<00:12, 354MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 10.8G/15.2G [00:31<00:14, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 10.9G/15.2G [00:31<00:13, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 11.0G/15.2G [00:31<00:09, 442MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 11.1G/15.2G [00:32<00:09, 439MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 11.2G/15.2G [00:32<00:08, 450MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 11.2G/15.2G [00:32<00:09, 423MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 11.3G/15.2G [00:32<00:08, 433MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ     | 11.4G/15.2G [00:32<00:08, 447MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 11.4G/15.2G [00:32<00:09, 397MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 11.5G/15.2G [00:33<00:10, 362MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 11.6G/15.2G [00:33<00:09, 390MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 11.6G/15.2G [00:33<00:08, 407MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 11.7G/15.2G [00:33<00:08, 427MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11.8G/15.2G [00:33<00:08, 383MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11.8G/15.2G [00:33<00:08, 406MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 11.9G/15.2G [00:34<00:07, 416MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 12.0G/15.2G [00:34<00:07, 431MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 12.0G/15.2G [00:34<00:08, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12.1G/15.2G [00:34<00:07, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12.2G/15.2G [00:34<00:07, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 12.3G/15.2G [00:35<00:06, 427MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12.3G/15.2G [00:35<00:07, 367MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12.4G/15.2G [00:35<00:07, 392MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 12.5G/15.2G [00:35<00:08, 302MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 12.6G/15.2G [00:35<00:07, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 12.6G/15.2G [00:36<00:07, 361MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 12.7G/15.2G [00:36<00:06, 393MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 12.8G/15.2G [00:36<00:07, 317MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 12.9G/15.2G [00:36<00:05, 398MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 13.0G/15.2G [00:36<00:06, 362MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 13.0G/15.2G [00:37<00:06, 355MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 13.1G/15.2G [00:37<00:05, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 13.2G/15.2G [00:37<00:05, 394MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 13.2G/15.2G [00:37<00:04, 404MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 13.3G/15.2G [00:38<00:06, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 13.4G/15.2G [00:38<00:05, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 13.4G/15.2G [00:38<00:04, 362MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 13.5G/15.2G [00:38<00:04, 360MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 13.6G/15.2G [00:38<00:04, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 13.7G/15.2G [00:38<00:02, 536MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13.8G/15.2G [00:39<00:03, 413MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13.8G/15.2G [00:39<00:03, 356MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 13.9G/15.2G [00:39<00:03, 382MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 14.0G/15.2G [00:39<00:02, 404MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 14.0G/15.2G [00:39<00:02, 420MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 14.1G/15.2G [00:39<00:02, 411MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 14.2G/15.2G [00:40<00:02, 428MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14.2G/15.2G [00:40<00:02, 446MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14.3G/15.2G [00:40<00:01, 449MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 14.4G/15.2G [00:40<00:02, 336MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 14.4G/15.2G [00:40<00:02, 343MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 14.5G/15.2G [00:40<00:01, 375MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 14.6G/15.2G [00:41<00:01, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 14.6G/15.2G [00:41<00:01, 319MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 14.8G/15.2G [00:41<00:00, 462MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 14.8G/15.2G [00:41<00:01, 330MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.2G/15.2G [00:42<00:00, 358MB/s]\u001b[A\u001b[A\n",
            "Download complete. Moving file to /workspace/classifier_chat/model.safetensors\n",
            "Fetching 8 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:42<00:00,  5.36s/it]\n",
            "/workspace/classifier_chat\n",
            "‚úì Download complete\n",
            "‚úì Model saved to persistent volume: /workspace/classifier_chat\n"
          ]
        }
      ],
      "source": [
        "# Download pre-merged model to PERSISTENT VOLUME (not ephemeral root disk!)\n",
        "import os\n",
        "\n",
        "# CRITICAL: Use /workspace for large files to avoid filling the container root disk\n",
        "CHECKPOINT_PATH = \"/workspace/classifier_chat\"\n",
        "\n",
        "!rm -rf {CHECKPOINT_PATH}\n",
        "print(f\"Downloading model (15.5 GB) to {CHECKPOINT_PATH}...\")\n",
        "!hf download Yida/classifier_chat --local-dir {CHECKPOINT_PATH}\n",
        "print(\"‚úì Download complete\")\n",
        "print(f\"‚úì Model saved to persistent volume: {CHECKPOINT_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Authenticated!\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "token = getpass(\"Enter your HuggingFace token: \")\n",
        "login(token=token)\n",
        "print(\"‚úì Authenticated!\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üóëÔ∏è  Clearing HuggingFace cache...\n",
            "  Deleting: /root/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
            "  ‚úì Cleared: /root/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\n",
            "\n",
            "‚úÖ Cache cleared! OLD model code will be downloaded on next load.\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: Run this cell, THEN immediately run the model loading cell!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# CRITICAL: Clear ALL HuggingFace cache for this model\n",
        "cache_locations = [\n",
        "    \"/workspace/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
        "    os.path.expanduser(\"~/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
        "    \"/workspace/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
        "    os.path.expanduser(\"~/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
        "]\n",
        "\n",
        "print(\"üóëÔ∏è  Clearing HuggingFace cache...\")\n",
        "cleared_any = False\n",
        "for cache_dir in cache_locations:\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(f\"  Deleting: {cache_dir}\")\n",
        "        try:\n",
        "            shutil.rmtree(cache_dir)\n",
        "            cleared_any = True\n",
        "            print(f\"  ‚úì Cleared: {cache_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Failed to clear {cache_dir}: {e}\")\n",
        "\n",
        "if cleared_any:\n",
        "    print(\"\\n‚úÖ Cache cleared! OLD model code will be downloaded on next load.\")\n",
        "else:\n",
        "    print(\"\\n‚úì No cache found. OLD model code will be downloaded on first load.\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Run this cell, THEN immediately run the model loading cell!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading classifier...\n",
            "Step 1: Loading tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83b6d0a42bb74ad4ad857b1fe2ce05cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff95c16b3aac4eaab2e0f67a593d4ca1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd88ba6c5e1e42b8bffd33cc3b7f72d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Tokenizer loaded\n",
            "Step 2: Loading base model from HuggingFace Hub...\n",
            "  (Loading with memory optimization)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e57c1bdbf0a64694905006ec12bcaa4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7103f1f867034b05a3ce098a72fc31aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2c8855c20b84a31ab0f49f1c2c7238a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26b83a07cd9c420eb27292d52afcff1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90c8f04401774b8dbec1da523f10bfeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Base model loaded\n",
            "Step 3: Loading fine-tuned weights from checkpoint...\n",
            "  Loading weights directly to GPU (memory-optimized)...\n",
            "‚úì Checkpoint weights loaded (directly to GPU)\n",
            "Step 4: Wrapping with LLM2Vec...\n",
            "Step 5: Adding classification head...\n",
            "‚úì Classification head loaded\n",
            "\n",
            "‚úÖ Model fully loaded on GPU!\n",
            "   VRAM used: 15.1 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "from llm2vec import LLM2Vec\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# CRITICAL: Use persistent volume paths to avoid filling ephemeral root disk\n",
        "CHECKPOINT_PATH = \"/workspace/classifier_chat\"\n",
        "OFFLOAD_PATH = \"/workspace/offload\"\n",
        "\n",
        "def load_classifier(checkpoint_path=CHECKPOINT_PATH, num_labels=5):\n",
        "    \"\"\"\n",
        "    Memory-optimized model loading that avoids the double-load RAM spike.\n",
        "    - Uses /workspace for all large files (persistent volume)\n",
        "    - Loads checkpoint weights directly to GPU to avoid CPU RAM exhaustion\n",
        "    \"\"\"\n",
        "    print(\"Loading classifier...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    base_model_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
        "    \n",
        "    # Ensure offload directory exists on persistent volume\n",
        "    os.makedirs(OFFLOAD_PATH, exist_ok=True)\n",
        "\n",
        "    print(f\"Step 1: Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model_id,\n",
        "        trust_remote_code=True,\n",
        "        token=True\n",
        "    )\n",
        "    print(\"‚úì Tokenizer loaded\")\n",
        "    \n",
        "    print(f\"Step 2: Loading base model from HuggingFace Hub...\")\n",
        "    print(\"  (Loading with memory optimization)\")\n",
        "    \n",
        "    # Load base model with optimized settings\n",
        "    base_model = AutoModel.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda:0\",  # Direct to GPU\n",
        "        offload_folder=OFFLOAD_PATH,  # Use persistent volume for offload\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        token=True\n",
        "    )\n",
        "    print(\"‚úì Base model loaded\")\n",
        "    \n",
        "    # Clear any CPU memory used during loading\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"Step 3: Loading fine-tuned weights from checkpoint...\")\n",
        "    checkpoint_file = os.path.join(checkpoint_path, \"model.safetensors\")\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        from safetensors.torch import load_file\n",
        "        \n",
        "        # CRITICAL FIX: Load weights directly to GPU to avoid CPU RAM spike\n",
        "        # This prevents the \"double load\" that exhausts System RAM\n",
        "        print(\"  Loading weights directly to GPU (memory-optimized)...\")\n",
        "        state_dict = load_file(checkpoint_file, device=\"cuda:0\")\n",
        "        \n",
        "        # Now assign weights - both model and state_dict are on GPU\n",
        "        base_model.load_state_dict(state_dict, strict=False)\n",
        "        \n",
        "        # Immediately free the state_dict to release VRAM\n",
        "        del state_dict\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        print(\"‚úì Checkpoint weights loaded (directly to GPU)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Checkpoint not found at {checkpoint_file}, using base model\")\n",
        "    \n",
        "    print(\"Step 4: Wrapping with LLM2Vec...\")\n",
        "    model = LLM2Vec(base_model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
        "    \n",
        "    print(\"Step 5: Adding classification head...\")\n",
        "    hidden_size = base_model.config.hidden_size\n",
        "    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n",
        "\n",
        "    head_file = os.path.join(checkpoint_path, \"head.pt\")\n",
        "    if os.path.exists(head_file):\n",
        "        target_device = torch.device(\"cuda:0\")\n",
        "        model.head.load_state_dict(torch.load(head_file, map_location=target_device, weights_only=True))\n",
        "        model.head = model.head.to(target_device)\n",
        "        print(\"‚úì Classification head loaded\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Head not found at {head_file}\")\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Final cleanup\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Model fully loaded on GPU!\")\n",
        "    print(f\"   VRAM used: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
        "    return model\n",
        "\n",
        "model = load_classifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_text(model, text):\n",
        "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
        "    \n",
        "    # Prepare & Tokenize\n",
        "    prepared_text = model.prepare_for_tokenization(text)\n",
        "    inputs = model.tokenize([prepared_text])\n",
        "    \n",
        "    # Device handling\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "    \n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.forward(inputs)\n",
        "        if hasattr(model, 'head'):\n",
        "            embeddings = embeddings.to(next(model.head.parameters()).device)\n",
        "        \n",
        "        embeddings = embeddings.to(torch.bfloat16)\n",
        "        probs = torch.nn.functional.softmax(model.head(embeddings), dim=-1)\n",
        "    \n",
        "    # Results\n",
        "    pred_idx = torch.argmax(probs, dim=-1).item()\n",
        "    all_probs = probs[0].float().cpu().numpy()\n",
        "    \n",
        "    print(f\"\\nPrediction: {label_names[pred_idx]} ({all_probs[pred_idx]*100:.2f}%)\")\n",
        "    sorted_idxs = np.argsort(all_probs)[::-1]\n",
        "    for i in sorted_idxs:\n",
        "        print(f\"{label_names[i]:10} {all_probs[i]*100:6.2f}% {'‚ñà' * int(all_probs[i]*20)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction: ChatGPT (36.13%)\n",
            "ChatGPT     36.13% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Grok        29.30% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "Claude      26.37% ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "DeepSeek     4.27% \n",
            "Gemini       3.91% \n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
        "1. First, it's important to understand the context\n",
        "2. Second, we should consider the implications\n",
        "\"\"\"\n",
        "predict_text(model, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://42865431bbb7ee278e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://42865431bbb7ee278e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import plotly.graph_objects as go\n",
        "import io\n",
        "\n",
        "def predict_gradio(text):\n",
        "    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Enter text to analyze\", None, \"‚ö†Ô∏è No text provided\"\n",
        "    \n",
        "    log_capture = io.StringIO()\n",
        "    \n",
        "    try:\n",
        "        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
        "        \n",
        "        log_capture.write(\"üîÑ Starting prediction...\\n\")\n",
        "        log_capture.write(f\"üìù Text length: {len(text)} characters\\n\")\n",
        "        \n",
        "        log_capture.write(\"\\nüî§ Tokenizing input...\\n\")\n",
        "        prepared_text = model.prepare_for_tokenization(text)\n",
        "        inputs = model.tokenize([prepared_text])\n",
        "        log_capture.write(\"‚úì Tokenization complete\\n\")\n",
        "        \n",
        "        # Dynamic device detection\n",
        "        try:\n",
        "            target_device = next(model.parameters()).device\n",
        "        except:\n",
        "            target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        log_capture.write(f\"\\nüñ•Ô∏è  Device: {target_device}\\n\")\n",
        "        \n",
        "        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "        \n",
        "        log_capture.write(\"\\nüß† Running model inference...\\n\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.forward(inputs)\n",
        "            log_capture.write(f\"‚úì Generated embeddings: {embeddings.shape}\\n\")\n",
        "            \n",
        "            if hasattr(model, 'head'):\n",
        "                head_device = next(model.head.parameters()).device\n",
        "                embeddings = embeddings.to(head_device)\n",
        "            \n",
        "            embeddings = embeddings.to(torch.bfloat16)\n",
        "            logits = model.head(embeddings)\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            log_capture.write(\"‚úì Computed probabilities\\n\")\n",
        "        \n",
        "        pred_label = torch.argmax(probabilities, dim=-1).item()\n",
        "        all_probs = probabilities[0].float().cpu().numpy()\n",
        "        \n",
        "        log_capture.write(f\"\\n{'='*40}\\n\")\n",
        "        log_capture.write(f\"üéØ Prediction: {label_names[pred_label]}\\n\")\n",
        "        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.1f}%\\n\")\n",
        "        log_capture.write(f\"{'='*40}\\n\\n\")\n",
        "        \n",
        "        sorted_indices = np.argsort(all_probs)[::-1]\n",
        "        log_capture.write(\"üìä All probabilities:\\n\")\n",
        "        for idx in sorted_indices:\n",
        "            bar = \"‚ñà\" * int(all_probs[idx] * 30)\n",
        "            log_capture.write(f\"  {label_names[idx]:12} {all_probs[idx]*100:5.1f}% {bar}\\n\")\n",
        "        \n",
        "        log_capture.write(\"\\n‚úÖ Analysis complete!\\n\")\n",
        "        \n",
        "        # Result text with clear formatting\n",
        "        result_text = f\"## Detected LLM: **{label_names[pred_label]}**\\n\\n### Confidence: **{all_probs[pred_label]*100:.1f}%**\"\n",
        "        \n",
        "        # Bar chart\n",
        "        sorted_labels = [label_names[i] for i in sorted_indices]\n",
        "        sorted_probs = [float(all_probs[i]) for i in sorted_indices]\n",
        "        \n",
        "        colors = ['#1f77b4' if i == 0 else '#aec7e8' for i in range(len(sorted_labels))]\n",
        "        \n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(\n",
        "                x=sorted_labels,\n",
        "                y=sorted_probs,\n",
        "                text=[f'{p*100:.1f}%' for p in sorted_probs],\n",
        "                textposition='outside',\n",
        "                marker_color=colors,\n",
        "                marker_line_width=0,\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        fig.update_layout(\n",
        "            xaxis_title=None,\n",
        "            yaxis_title=None,\n",
        "            yaxis=dict(range=[0, max(sorted_probs) * 1.15], showticklabels=False, showgrid=False),\n",
        "            xaxis=dict(showgrid=False),\n",
        "            height=200,\n",
        "            margin=dict(l=10, r=10, t=10, b=30),\n",
        "            showlegend=False,\n",
        "            plot_bgcolor='white',\n",
        "            paper_bgcolor='white',\n",
        "        )\n",
        "        \n",
        "        return result_text, fig, log_capture.getvalue()\n",
        "        \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "        log_capture.write(error_msg)\n",
        "        \n",
        "        empty_fig = go.Figure()\n",
        "        empty_fig.update_layout(height=200)\n",
        "        return f\"Error: {str(e)}\", empty_fig, log_capture.getvalue()\n",
        "\n",
        "\n",
        "# Single viewport UI with logs\n",
        "with gr.Blocks(title=\"Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\") as demo:\n",
        "    gr.Markdown(\"# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\")\n",
        "    gr.Markdown(\"**[Research Paper](https://eric-mingjie.github.io/llm-idiosyncrasies/index.html)** (97% accuracy) ‚Ä¢ **[GitHub](https://github.com/syedamaann/llm-idiosyncrasies)** ‚Ä¢ **[syedamaan.com](https://syedamaan.com)**\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        # Left: Input\n",
        "        with gr.Column(scale=1):\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"Input Text\",\n",
        "                placeholder=\"Paste text here...\",\n",
        "                lines=8,\n",
        "                max_lines=8,\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\", size=\"lg\")\n",
        "        \n",
        "        # Right: Results and Chart\n",
        "        with gr.Column(scale=1):\n",
        "            result_output = gr.Markdown(value=\"**Results will appear here**\")\n",
        "            plot_output = gr.Plot()\n",
        "    \n",
        "    # Bottom: Processing logs (compact)\n",
        "    logs_output = gr.Textbox(\n",
        "        label=\"Processing Log\",\n",
        "        lines=8,\n",
        "        max_lines=8,\n",
        "        interactive=False,\n",
        "    )\n",
        "    \n",
        "    submit_btn.click(\n",
        "        fn=predict_gradio,\n",
        "        inputs=text_input,\n",
        "        outputs=[result_output, plot_output, logs_output]\n",
        "    )\n",
        "    \n",
        "    text_input.submit(\n",
        "        fn=predict_gradio,\n",
        "        inputs=text_input,\n",
        "        outputs=[result_output, plot_output, logs_output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
