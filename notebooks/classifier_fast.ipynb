{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classifier\n",
        "\n",
        "Lightweight version using pre-merged weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -q transformers==4.44.2 peft==0.13.2 huggingface-hub accelerate safetensors\n",
        "%pip install -q llm2vec==0.2.3 gradio plotly hf_transfer\n",
        "\n",
        "print(\"‚úì Dependencies installed\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Restart the kernel before continuing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "assert transformers.__version__ == \"4.44.2\", f\"Need transformers==4.44.2 for llm2vec compatibility, got {transformers.__version__}\"\n",
        "print(\"‚úì Version check passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download pre-merged model\n",
        "import os\n",
        "\n",
        "!rm -rf ./classifier_chat\n",
        "print(\"Downloading model (15.5 GB)...\")\n",
        "!hf download Yida/classifier_chat --local-dir ./classifier_chat\n",
        "print(\"‚úì Download complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# CRITICAL: Clear ALL HuggingFace cache for this model\n",
        "cache_locations = [\n",
        "    \"/workspace/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
        "    os.path.expanduser(\"~/.cache/huggingface/modules/transformers_modules/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
        "    \"/workspace/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\",\n",
        "    os.path.expanduser(\"~/.cache/huggingface/hub/models--McGill-NLP--LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"),\n",
        "]\n",
        "\n",
        "print(\"üóëÔ∏è  Clearing HuggingFace cache...\")\n",
        "cleared_any = False\n",
        "for cache_dir in cache_locations:\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(f\"  Deleting: {cache_dir}\")\n",
        "        try:\n",
        "            shutil.rmtree(cache_dir)\n",
        "            cleared_any = True\n",
        "            print(f\"  ‚úì Cleared: {cache_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Failed to clear {cache_dir}: {e}\")\n",
        "\n",
        "if cleared_any:\n",
        "    print(\"\\n‚úÖ Cache cleared! OLD model code will be downloaded on next load.\")\n",
        "else:\n",
        "    print(\"\\n‚úì No cache found. OLD model code will be downloaded on first load.\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Run this cell, THEN immediately run the model loading cell!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "from llm2vec import LLM2Vec\n",
        "import gc\n",
        "import os\n",
        "\n",
        "def load_classifier(checkpoint_path=\"./classifier_chat\", num_labels=5):\n",
        "    print(\"Loading classifier...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    base_model_id = \"McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp\"\n",
        "    max_memory = {0: \"14GiB\", \"cpu\": \"30GiB\"}\n",
        "\n",
        "    print(f\"Step 1: Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        base_model_id,\n",
        "        trust_remote_code=True,\n",
        "        token=True  # Use stored HF token for authentication\n",
        "    )\n",
        "    print(\"‚úì Tokenizer loaded\")\n",
        "    \n",
        "    print(f\"Step 2: Loading base model from HuggingFace Hub...\")\n",
        "    print(\"  (Loading latest compatible version)\")\n",
        "    \n",
        "    # Load base model without specific revision\n",
        "    # The pre-merged weights from classifier_chat will be loaded on top\n",
        "    base_model = AutoModel.from_pretrained(\n",
        "        base_model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map={\"\": 0},\n",
        "        max_memory=max_memory,\n",
        "        offload_folder=\"./offload\",\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        token=True  # Use stored HF token for authentication\n",
        "    )\n",
        "    print(\"‚úì Base model loaded\")\n",
        "    \n",
        "    print(\"Step 3: Loading fine-tuned weights from checkpoint...\")\n",
        "    checkpoint_file = os.path.join(checkpoint_path, \"model.safetensors\")\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        from safetensors.torch import load_file\n",
        "        state_dict = load_file(checkpoint_file)\n",
        "        base_model.load_state_dict(state_dict, strict=False)\n",
        "        print(\"‚úì Checkpoint weights loaded\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Checkpoint not found at {checkpoint_file}, using base model\")\n",
        "    \n",
        "    print(\"Step 4: Wrapping with LLM2Vec...\")\n",
        "    model = LLM2Vec(base_model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
        "    \n",
        "    print(\"Step 5: Adding classification head...\")\n",
        "    # Get hidden size from model config instead of accessing module weights\n",
        "    hidden_size = base_model.config.hidden_size\n",
        "    model.head = torch.nn.Linear(hidden_size, num_labels, dtype=torch.bfloat16)\n",
        "\n",
        "    head_file = os.path.join(checkpoint_path, \"head.pt\")\n",
        "    if os.path.exists(head_file):\n",
        "        try:\n",
        "            target_device = next(model.parameters()).device\n",
        "        except:\n",
        "            target_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        model.head.load_state_dict(torch.load(head_file, map_location=target_device))\n",
        "        model.head = model.head.to(target_device)\n",
        "        print(\"‚úì Classification head loaded\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Head not found at {head_file}\")\n",
        "    \n",
        "    model.eval()\n",
        "    print(f\"\\n‚úÖ Model fully loaded on {'GPU' if torch.cuda.is_available() else 'CPU'}!\")\n",
        "    return model\n",
        "\n",
        "model = load_classifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_text(model, text):\n",
        "    label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
        "    \n",
        "    # Prepare & Tokenize\n",
        "    prepared_text = model.prepare_for_tokenization(text)\n",
        "    inputs = model.tokenize([prepared_text])\n",
        "    \n",
        "    # Device handling\n",
        "    try:\n",
        "        device = next(model.parameters()).device\n",
        "    except:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "    \n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.forward(inputs)\n",
        "        if hasattr(model, 'head'):\n",
        "            embeddings = embeddings.to(next(model.head.parameters()).device)\n",
        "        \n",
        "        embeddings = embeddings.to(torch.bfloat16)\n",
        "        probs = torch.nn.functional.softmax(model.head(embeddings), dim=-1)\n",
        "    \n",
        "    # Results\n",
        "    pred_idx = torch.argmax(probs, dim=-1).item()\n",
        "    all_probs = probs[0].float().cpu().numpy()\n",
        "    \n",
        "    print(f\"\\nPrediction: {label_names[pred_idx]} ({all_probs[pred_idx]*100:.2f}%)\")\n",
        "    sorted_idxs = np.argsort(all_probs)[::-1]\n",
        "    for i in sorted_idxs:\n",
        "        print(f\"{label_names[i]:10} {all_probs[i]*100:6.2f}% {'‚ñà' * int(all_probs[i]*20)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Hello! I'd be happy to help you with that question. Let me break this down into a few key points:\n",
        "1. First, it's important to understand the context\n",
        "2. Second, we should consider the implications\n",
        "\"\"\"\n",
        "predict_text(model, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import plotly.graph_objects as go\n",
        "import io\n",
        "\n",
        "def predict_gradio(text):\n",
        "    \"\"\"Predict for Gradio interface with detailed logs.\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Enter text to analyze\", None, \"‚ö†Ô∏è No text provided\"\n",
        "    \n",
        "    log_capture = io.StringIO()\n",
        "    \n",
        "    try:\n",
        "        label_names = [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\", \"DeepSeek\"]\n",
        "        \n",
        "        log_capture.write(\"üîÑ Starting prediction...\\n\")\n",
        "        log_capture.write(f\"üìù Text length: {len(text)} characters\\n\")\n",
        "        \n",
        "        log_capture.write(\"\\nüî§ Tokenizing input...\\n\")\n",
        "        prepared_text = model.prepare_for_tokenization(text)\n",
        "        inputs = model.tokenize([prepared_text])\n",
        "        log_capture.write(\"‚úì Tokenization complete\\n\")\n",
        "        \n",
        "        # Dynamic device detection\n",
        "        try:\n",
        "            target_device = next(model.parameters()).device\n",
        "        except:\n",
        "            target_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        log_capture.write(f\"\\nüñ•Ô∏è  Device: {target_device}\\n\")\n",
        "        \n",
        "        inputs = {k: v.to(target_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "        \n",
        "        log_capture.write(\"\\nüß† Running model inference...\\n\")\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.forward(inputs)\n",
        "            log_capture.write(f\"‚úì Generated embeddings: {embeddings.shape}\\n\")\n",
        "            \n",
        "            if hasattr(model, 'head'):\n",
        "                head_device = next(model.head.parameters()).device\n",
        "                embeddings = embeddings.to(head_device)\n",
        "            \n",
        "            embeddings = embeddings.to(torch.bfloat16)\n",
        "            logits = model.head(embeddings)\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            log_capture.write(\"‚úì Computed probabilities\\n\")\n",
        "        \n",
        "        pred_label = torch.argmax(probabilities, dim=-1).item()\n",
        "        all_probs = probabilities[0].float().cpu().numpy()\n",
        "        \n",
        "        log_capture.write(f\"\\n{'='*40}\\n\")\n",
        "        log_capture.write(f\"üéØ Prediction: {label_names[pred_label]}\\n\")\n",
        "        log_capture.write(f\"üíØ Confidence: {all_probs[pred_label]*100:.1f}%\\n\")\n",
        "        log_capture.write(f\"{'='*40}\\n\\n\")\n",
        "        \n",
        "        sorted_indices = np.argsort(all_probs)[::-1]\n",
        "        log_capture.write(\"üìä All probabilities:\\n\")\n",
        "        for idx in sorted_indices:\n",
        "            bar = \"‚ñà\" * int(all_probs[idx] * 30)\n",
        "            log_capture.write(f\"  {label_names[idx]:12} {all_probs[idx]*100:5.1f}% {bar}\\n\")\n",
        "        \n",
        "        log_capture.write(\"\\n‚úÖ Analysis complete!\\n\")\n",
        "        \n",
        "        # Result text with clear formatting\n",
        "        result_text = f\"## Detected LLM: **{label_names[pred_label]}**\\n\\n### Confidence: **{all_probs[pred_label]*100:.1f}%**\"\n",
        "        \n",
        "        # Bar chart\n",
        "        sorted_labels = [label_names[i] for i in sorted_indices]\n",
        "        sorted_probs = [float(all_probs[i]) for i in sorted_indices]\n",
        "        \n",
        "        colors = ['#1f77b4' if i == 0 else '#aec7e8' for i in range(len(sorted_labels))]\n",
        "        \n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(\n",
        "                x=sorted_labels,\n",
        "                y=sorted_probs,\n",
        "                text=[f'{p*100:.1f}%' for p in sorted_probs],\n",
        "                textposition='outside',\n",
        "                marker_color=colors,\n",
        "                marker_line_width=0,\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        fig.update_layout(\n",
        "            xaxis_title=None,\n",
        "            yaxis_title=None,\n",
        "            yaxis=dict(range=[0, max(sorted_probs) * 1.15], showticklabels=False, showgrid=False),\n",
        "            xaxis=dict(showgrid=False),\n",
        "            height=200,\n",
        "            margin=dict(l=10, r=10, t=10, b=30),\n",
        "            showlegend=False,\n",
        "            plot_bgcolor='white',\n",
        "            paper_bgcolor='white',\n",
        "        )\n",
        "        \n",
        "        return result_text, fig, log_capture.getvalue()\n",
        "        \n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"‚ùå Error: {str(e)}\\n\\n{traceback.format_exc()}\"\n",
        "        log_capture.write(error_msg)\n",
        "        \n",
        "        empty_fig = go.Figure()\n",
        "        empty_fig.update_layout(height=200)\n",
        "        return f\"Error: {str(e)}\", empty_fig, log_capture.getvalue()\n",
        "\n",
        "\n",
        "# Single viewport UI with logs\n",
        "with gr.Blocks(title=\"Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\") as demo:\n",
        "    gr.Markdown(\"# Which LLM Wrote This? ChatGPT, Claude, Gemini, or Grok?\")\n",
        "    gr.Markdown(\"**[Research Paper](https://eric-mingjie.github.io/llm-idiosyncrasies/index.html)** (97% accuracy) ‚Ä¢ **[GitHub](https://github.com/syedamaann/llm-idiosyncrasies)** ‚Ä¢ **[syedamaan.com](https://syedamaan.com)**\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        # Left: Input\n",
        "        with gr.Column(scale=1):\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"Input Text\",\n",
        "                placeholder=\"Paste text here...\",\n",
        "                lines=8,\n",
        "                max_lines=8,\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\", size=\"lg\")\n",
        "        \n",
        "        # Right: Results and Chart\n",
        "        with gr.Column(scale=1):\n",
        "            result_output = gr.Markdown(value=\"**Results will appear here**\")\n",
        "            plot_output = gr.Plot()\n",
        "    \n",
        "    # Bottom: Processing logs (compact)\n",
        "    logs_output = gr.Textbox(\n",
        "        label=\"Processing Log\",\n",
        "        lines=8,\n",
        "        max_lines=8,\n",
        "        interactive=False,\n",
        "    )\n",
        "    \n",
        "    submit_btn.click(\n",
        "        fn=predict_gradio,\n",
        "        inputs=text_input,\n",
        "        outputs=[result_output, plot_output, logs_output]\n",
        "    )\n",
        "    \n",
        "    text_input.submit(\n",
        "        fn=predict_gradio,\n",
        "        inputs=text_input,\n",
        "        outputs=[result_output, plot_output, logs_output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
